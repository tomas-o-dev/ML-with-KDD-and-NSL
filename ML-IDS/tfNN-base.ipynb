{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Essential ML process for Intrusion Detection**\n",
    "<br>` python  3.7.13    scikit-learn  1.0.2 `\n",
    "<br>`numpy   1.19.5          pandas  1.3.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S34U5S-i69d"
   },
   "source": [
    "**Import the main libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "\n",
    "from time import time\n",
    "trs = time()\n",
    "\n",
    "import os\n",
    "data_path = '../datasets/NSL_KDD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "_import the local library_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add parent folder path where lib folder is\n",
    "import sys\n",
    "if \"..\" not in sys.path:import sys; sys.path.insert(0, '..') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mylib import show_labels_dist, show_metrics, bias_var_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IZetEZ8jQJm"
   },
   "source": [
    "**Import the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using boosted Train and preprocessed Test\n",
    "\n",
    "data_file = os.path.join(data_path, 'NSL_boosted-1.csv') \n",
    "train_df = pandas.read_csv(data_file)\n",
    "print('Train Dataset: {} rows, {} columns'.format(train_df.shape[0], train_df.shape[1]))\n",
    "\n",
    "data_file = os.path.join(data_path, 'NSL_ppTest.csv') \n",
    "test_df = pandas.read_csv(data_file)\n",
    "print('Test Dataset: {} rows, {} columns'.format(test_df.shape[0], test_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Data Preparation and EDA** (unique to this dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _let's skip the Checking (EDA)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Combine for processing classification target and text features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df = pandas.concat([train_df, test_df])\n",
    "print('Combined Dataset: {} rows, {} columns'.format(\n",
    "    combined_df.shape[0], combined_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Classification Target feature:_\n",
    "two columns of labels are available \n",
    "    * Two-class: Reduce the detailed attack labels to 'normal' or 'attack'\n",
    "    * Multiclass: Use the category labels (atakcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the classification target\n",
    "twoclass = False     # True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if twoclass:\n",
    "# Two-class: Reduce the detailed attack labels to 'normal' or 'attack'\n",
    "# new single column data structure is a [series]\n",
    "    labels_df = combined_df['label'].copy()\n",
    "    labels_df[labels_df != 'normal'] = 'attack'\n",
    "else:\n",
    "# Multiclass: Use the category labels (atakcat)\n",
    "# new single column data structure is a [[dataframe]]\n",
    "# rename the column and convert to a series for later\n",
    "    labels_df = combined_df[['atakcat']].copy()\n",
    "    labels_df.rename(columns={'atakcat':'label'}, inplace=True)\n",
    "    labels_df = labels_df.squeeze('columns')\n",
    "\n",
    "# drop target features \n",
    "combined_df.drop(['label'], axis=1, inplace=True)\n",
    "combined_df.drop(['atakcat'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* _One-Hot Encoding the remaining categorical (text) features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put the names into a python list - for pandas.get_dummies()\n",
    "categori = combined_df.select_dtypes(include=['object']).columns\n",
    "category_cols = categori.tolist()\n",
    "print(category_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply to the list of Categorical columns (text fields)\n",
    "features_df = pandas.get_dummies(combined_df, columns=category_cols)\n",
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate a list of numeric columns for scaling - After test // train split\n",
    "numeri = combined_df.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "**<br>Create Test // Train Datasets**\n",
    "> Normally we split the dataset into train 70 % // test 30 % like this\n",
    "<br>`from sklearn.model_selection import train_test_split`\n",
    "<br>`X_train, X_test, y_train, y_test = `\n",
    "<br>`    train_test_split(features_df, labels_df, `\n",
    "<br>`        test_size=0.3, stratify=labels_df, random_state=42)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restore the train // test split: slice 1 Dataframe into 2 \n",
    "features_train = features_df.iloc[:len(train_df),:].copy()    # X_train\n",
    "features_train.reset_index(inplace=True, drop=True)\n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy from slice\n",
    "# so we force it to create a new dataframe [avoiding SettingWithCopy Warning]\n",
    "features_test = features_df.iloc[len(train_df):,:].copy()     # X_test\n",
    "features_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Restore the train // test split: slice 1 Series into 2 \n",
    "labels_train = labels_df[:len(train_df)]               # y_train\n",
    "labels_train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "labels_test = labels_df[len(train_df):]                # y_test\n",
    "labels_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "Next are standard steps for all datasets: _scaling, classifiers, results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**Scaling** comes _after_ test // train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scaling the Numeric columns \n",
    "# StandardScaler range: -1 to 1, MinMaxScaler range: zero to 1\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# sklearn docs say \n",
    "#   \"Don't cheat - fit only on training data, then transform both\"\n",
    "#   fit() expects 2D array: reshape(-1, 1) for single col or (1, -1) single row\n",
    "\n",
    "for i in numeri:\n",
    "    arr = numpy.array(features_train[i])\n",
    "    scale = MinMaxScaler().fit(arr.reshape(-1, 1))\n",
    "    features_train[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "\n",
    "    arr = numpy.array(features_test[i])\n",
    "    features_test[i] = scale.transform(arr.reshape(len(arr),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Classifier Selection<br>Fit and Predict**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**Tensorflow.Keras Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires numeric labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# generate a sorted list of the text labels to use later\n",
    "clss = list(numpy.sort(labels_test.unique()))\n",
    "\n",
    "## Feature being predicted (\"the Right Answer\")\n",
    "ytrain = LabelEncoder().fit_transform(labels_train)\n",
    "ytest = LabelEncoder().fit_transform(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard dataset names\n",
    "X_train = features_train\n",
    "X_test = features_test\n",
    "y_train = ytrain\n",
    "y_test = ytest\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Essential Hyperparameters\n",
    "CLASSES = 5          # output layer size\n",
    "EPOCHS = 10          # max runs through the network\n",
    "BATCH_SIZE = 256     # training data subset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Define the model** (see NNmodelDefs.ipynb for pop-in definitions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**tensorflow.keras \"feed forward\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape[0] = rows|observations ; shape[1] = cols|features\n",
    "# shape for initial input tensor depends on first layer:\n",
    "#     Dense (Feed Forward|Fully Connected) uses 2D\n",
    "#     CNN1D, RNN both use 3D (with different semantics for the 3rd dim!)\n",
    "\n",
    "# Dense initial layer: no need to reshape ... \n",
    "shape = (X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dense layer = Feed Forward|Fully Connected \n",
    "# If you don't specify an Activation function, no activation is applied \n",
    "#   (ie. \"linear\" activation: a(x) = x).\n",
    "\n",
    "# NO Spaces in names\n",
    "model_name = 'feed_forward'\n",
    "\n",
    "model = keras.Sequential()\n",
    "# use the proper shape!\n",
    "model.add(keras.layers.InputLayer(input_shape=shape, name='optionalLayer'))\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation='relu', name='InitialLayer'))\n",
    "model.add(keras.layers.Dense(64, activation='relu', name='mid_Layer'))\n",
    "model.add(keras.layers.Dense(32, activation='relu', name=\"mid-Layer\"))\n",
    "\n",
    "# output layers\n",
    "model.add(keras.layers.Dense(CLASSES, name=\"OutputLayer\"))\n",
    "model.add(keras.layers.Softmax(name=\"ResultLayer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<br>Below works for any Tensorflow.Keras model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saves a picture of the model to results\n",
    "# rankdir 'TB' creates a vertical plot; 'LR' creates a horizontal plot.\n",
    "keras.utils.plot_model(model,\n",
    "                       f'{model_name}_graph.png',\n",
    "                       show_shapes=True,\n",
    "                       show_layer_names=True,\n",
    "                       rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Fit the model to the training data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "              optimizer = \"adam\",\n",
    "              metrics = ['acc','mse']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the training when there is no improvement in the\n",
    "# loss (min_delta) for three consecutive epochs (patience)\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
    "                        patience=3, verbose=1, mode='auto', \n",
    "                        restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, \n",
    "                 epochs=EPOCHS, \n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 # validation_data=(X_test,y_test),\n",
    "                 validation_split = .15,\n",
    "                 # callbacks=[monitor],\n",
    "                 shuffle = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# model.save(f'{model_name}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Show validation statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line\n",
    "train_loss, train_accuracy, train_mse = model.evaluate(X_train,  y_train, verbose=1)\n",
    "test_loss, test_accuracy, test_mse = model.evaluate(X_test,  y_test, verbose=1)\n",
    "\n",
    "#print('Elapsed %.3f seconds.' % elapsed)\n",
    "print('****** TRAIN ******')\n",
    "print('Loss : %.4f   Accuracy : %.4f' % (train_loss, train_accuracy))\n",
    "#print(f\"Loss: {train_loss}\\nAccuracy: {train_accuracy}\")\n",
    "print('****** TEST ******')\n",
    "print('Loss : %.4f   Accuracy : %.4f' % (test_loss, test_accuracy))\n",
    "#print('Loss :  %.3f' % test_loss)\n",
    "#print(f\"Loss: {test_loss}\\nAccuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize = (20,10))\n",
    "plt.plot(hist.history['acc'], label = 'Train')\n",
    "plt.plot(hist.history['val_acc'], label='Validation')\n",
    "plt.title(\"Train Accuracy vs Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks([i for i in range(EPOCHS)])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(f'{model_name} - accuracy')\n",
    "#plt.savefig(f'{model_name}_accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize = (20,10))\n",
    "plt.plot(hist.history['loss'], label = 'Train')\n",
    "plt.plot(hist.history['val_loss'], label='Validation')\n",
    "plt.title(\"Train Loss vs Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks([i for i in range(EPOCHS)])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(f'{model_name} - loss')\n",
    "#plt.savefig(f'{model_name}_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Get predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([X_test])\n",
    "predictions = y_pred.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _Convert everything back to text labels for our metrics function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert predictions and 'ground truth'\n",
    "ll_pred = pandas.DataFrame(predictions)\n",
    "ll_pred[0] = ll_pred[0].apply(lambda x: clss[x])\n",
    "\n",
    "ll_ytst = pandas.DataFrame(y_test)\n",
    "ll_ytst[0] = ll_ytst[0].apply(lambda x: clss[x])\n",
    "\n",
    "print('macro average: unweighted mean per label')\n",
    "print('weighted average: support-weighted mean per label')\n",
    "print('MCC: correlation between prediction and ground truth')\n",
    "print('     (+1 perfect, 0 random prediction, -1 inverse)')\n",
    "\n",
    "print(\"~~~~\\n\")\n",
    "print('Confusion Matrix:', model_name)\n",
    "show_metrics(ll_ytst,ll_pred,clss)    # from our local library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
