{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Essential ML process for Intrusion Detection**\n",
    "<br>`NOTE: {python3.8 numpy 1.19.5} are max versions for the June 2022 conda tensorflow 2.2-2.6 builds (at least) - seems like the pip build works with numpy >= 1.20 but pip install breaks the consistency of the conda environment`<br>This was fixed in the Dec.2022 builds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**In This Notebook:**<br>\n",
    "* Model Definitions\n",
    "* Using Class Weights\n",
    "* Notes on Using Class Weights\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Model Definitions**<br>\n",
    "* tensorflow.keras \"feed forward\"\n",
    "* tensorflow.keras RNN\n",
    "* tensorflow.keras LSTM\n",
    "* tensorflow.keras Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**tensorflow.keras \"feed forward\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape[0] = rows|observations ; shape[1] = cols|features\n",
    "# shape for initial input tensor depends on first layer:\n",
    "#     Dense (Feed Forward|Fully Connected) uses 2D\n",
    "#     CNN1D, RNN both use 3D (with different semantics for the 3rd dim!)\n",
    "\n",
    "# Dense initial layer: no need to reshape ... \n",
    "shape = (X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dense layer = Feed Forward|Fully Connected \n",
    "# If you don't specify an Activation function, no activation is applied \n",
    "#   (ie. \"linear\" activation: a(x) = x).\n",
    "\n",
    "# NO Spaces in names\n",
    "model_name = 'feed_forward'\n",
    "\n",
    "model = keras.Sequential()\n",
    "# use the proper shape!\n",
    "model.add(keras.layers.InputLayer(input_shape=shape, name='optionalLayer'))\n",
    "\n",
    "model.add(keras.layers.Dense(128, activation='relu', name='InitialLayer'))\n",
    "model.add(keras.layers.Dense(64, activation='relu', name='mid_Layer'))\n",
    "model.add(keras.layers.Dense(32, activation='relu', name=\"mid-Layer\"))\n",
    "\n",
    "# output layers\n",
    "model.add(keras.layers.Dense(CLASSES, name=\"OutputLayer\"))\n",
    "model.add(keras.layers.Softmax(name=\"ResultLayer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "**tensorflow.keras RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape[0] = rows|observations ; shape[1] = cols|features\n",
    "# shape for initial input tensor depends on first layer:\n",
    "#     Dense (Feed Forward|Fully Connected) uses 2D\n",
    "#     CNN1D, RNN both use 3D (with different semantics for the 3rd dim!)\n",
    "\n",
    "# reshape the datasets to 3D\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# shape for initial input tensor: CNN1D, RNN \n",
    "shape = (X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recurrent layers (RNN, LSTM) require \n",
    "#    return_sequences=True\n",
    "# to initialise another Recurrent layer\n",
    "# shape of this output is (batch_size, timesteps, units).\n",
    "# for a Dense layer, the default (False) is fine\n",
    "# shape of this output is (batch_size, units) \n",
    "#    where units corresponds to the argument passed to the constructor\n",
    "\n",
    "model_name = 'RNN'\n",
    "model = keras.Sequential()\n",
    "# use the proper shape!\n",
    "model.add(keras.layers.InputLayer(input_shape = shape))\n",
    "\n",
    "model.add(keras.layers.SimpleRNN(128, return_sequences=True))\n",
    "model.add(keras.layers.SimpleRNN(64))\n",
    "\n",
    "# output layers\n",
    "model.add(keras.layers.Dense(CLASSES, name=\"OutputLayer\"))\n",
    "model.add(keras.layers.Softmax(name=\"ResultLayer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "**tensorflow.keras LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape[0] = rows|observations ; shape[1] = cols|features\n",
    "# shape for initial input tensor depends on first layer:\n",
    "#     Dense (Feed Forward|Fully Connected) uses 2D\n",
    "#     CNN1D, RNN both use 3D (with different semantics for the 3rd dim!)\n",
    "\n",
    "# reshape the datasets to 3D\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# shape for initial input tensor: CNN1D, RNN \n",
    "shape = (X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent layers (RNN, LSTM) require \n",
    "#    return_sequences=True\n",
    "# to initialise another Recurrent layer\n",
    "# shape of this output is (batch_size, timesteps, units).\n",
    "# for a Dense layer, the default (False) is fine\n",
    "# shape of this output is (batch_size, units) \n",
    "#    where units corresponds to the argument passed to the constructor\n",
    "\n",
    "model_name = 'lstm'\n",
    "model = keras.Sequential()\n",
    "# use the proper shape!\n",
    "model.add(keras.layers.InputLayer(input_shape = shape))\n",
    "\n",
    "model.add(keras.layers.LSTM(128, return_sequences=True)) \n",
    "model.add(keras.layers.LSTM(64))\n",
    "\n",
    "# output layers\n",
    "model.add(keras.layers.Dense(CLASSES, name=\"OutputLayer\"))\n",
    "model.add(keras.layers.Softmax(name=\"ResultLayer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**tensorflow.keras Conv1D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape[0] = rows|observations ; shape[1] = cols|features\n",
    "# shape for initial input tensor depends on first layer:\n",
    "#     Dense (ANN|Fully Connected) uses 2D\n",
    "#     CNN1D, RNN both use 3D (with different semantics for the 3rd dim!)\n",
    "\n",
    "# reshape the datasets to 3D\n",
    "X_train = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# shape for initial input tensor: CNN1D, RNN \n",
    "shape = (X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'conv1D'\n",
    "model = keras.Sequential()\n",
    "# use the proper shape!\n",
    "model.add(keras.layers.InputLayer(input_shape = shape))\n",
    "\n",
    "model.add(keras.layers.Conv1D(filters = 64,\n",
    "                              kernel_size = 4, strides = 1,\n",
    "                              padding = 'valid'))\n",
    "model.add(tf.keras.layers.LSTM(64))  \n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "# output layers\n",
    "model.add(keras.layers.Dense(CLASSES, name=\"OutputLayer\"))\n",
    "model.add(keras.layers.Softmax(name=\"ResultLayer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Using Class Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced weighting is one of the widely used methods for imbalanced classification models. It modifies the class weights of the majority and minority classes during the model training process to achieve better model results.\n",
    "\n",
    "Unlike the oversampling and under-sampling methods, the balanced weighting methods do not modify the dataset. Instead, each observation is weighted so that wrong predictions for the minority class are given more weight when the loss value is calculated during the training process. Weights for the loss function by can be arbitrary, but a typical choice is class weights (distribution of labels). \n",
    "\n",
    "Let's start with the practical requirements for using class weights in our model, with an explanation of terms and concepts after. The blocks below can be used to make the changes to our typical example neural network. There are four things to take care of:\n",
    "\n",
    "> 1a. No keras.Softmax() layer at the end of the model definition<br>\n",
    "1b. Define the loss function with the parameter from_logits=True<br><br>\n",
    "2a. Calculate the weights<br>\n",
    "2b. Choose whether we pass the weights to model.fit() or model.compile()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a. No keras.Softmax() layer at the end of the model definition**\n",
    "<br>Many multi-layer neural networks end with a Softmax() layer, to convert real-valued scores (\"logits\") to a normalized probability distribution that is more convenient for display to users and passing to other programs. However, <a href=\"https://www.tensorflow.org/tutorials/quickstart/beginner\"> the tensorflow docs</a> point out that it is impossible to provide an exact and numerically stable loss calculation for all models when using a softmax output.\n",
    "\n",
    "So, step 1a is to modify our model definition block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output layers\n",
    "model.add(keras.layers.Dense(CLASSES, name=\"OutputLayer\"))\n",
    "# model.add(keras.layers.Softmax(name=\"ResultLayer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b. Define the loss function with the parameter from_logits=True**\n",
    "<br>tf.keras built-in loss functions may be passed by their string identifier, or by \n",
    "instantiating a loss class. Using classes enables you to pass configuration arguments at instantiation time, using the string identifier is more convenient when all you need are the default parameters.\n",
    "\n",
    "The default in the tf.keras loss function definitions is to assume the optimizer will use softmax inputs. To use class weights, we need to tell it that the inputs will be \"logits\" to preserve accuracy, which means using the class instantiation method.\n",
    "\n",
    "It is probably easiest to just replace the model.compile() block ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "model.compile(loss = 'sparse_categorical_crossentropy', \n",
    "              optimizer = \"adam\",\n",
    "              metrics = ['acc','mse']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_weight =   causes error with keras.Softmax as last model layer\n",
    "#                * with no Softmax, we must tell the loss function to use logits\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss = loss_fn,\n",
    "              # loss_weights = loss_wts,\n",
    "              optimizer = \"adam\",\n",
    "              metrics = ['acc','mse']\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a. Calculate the weights**\n",
    "<br>Scikit-Learn has a convenient \n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\"> compute_class_weight</a> function that makes this painless. With class_weight = balanced, the weights are calculated as <br>\n",
    "> n_samples / (n_classes * np.bincount(y))\n",
    "\n",
    "Just to note, the function does not support positional parameters, we need to specify the keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this block\n",
    "# just above the model.compile() block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.37587241246176234,\n",
       " 1: 0.5486863782190237,\n",
       " 2: 2.1425427458947013,\n",
       " 3: 21.896193771626297,\n",
       " 4: 218.20689655172413}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "loss_wts = compute_class_weight(class_weight = \"balanced\",\n",
    "                                classes = numpy.unique(y_train),\n",
    "                                y = y_train\n",
    "                               )\n",
    "# model.compile() just needs the array for loss_weights =\n",
    "# model.fit() requires a dict for class_weight = \n",
    "clas_wts = dict(zip(numpy.unique(y_train), loss_wts))\n",
    "clas_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b. Choose whether to pass the weights to model.fit() or model.compile()**\n",
    "<br>Passing a class_weight argument to model.fit() is used to weight the importance of each sample based on the class they belong to during training. This is typically used when there is an uneven distribution of samples per class.\n",
    "\n",
    "Passing a loss_weights argument to model.fit() is used to weight the multiple loss values in the calculation of the final loss value of the model. This is typically used for models with multiple loss functions (see the note below), but it can also be used for simple multiclass models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the early stopping block\n",
    "# replace the model.fit() block with this, or just add the line\n",
    "##     class_weight = clas_wts,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, \n",
    "                 epochs=EPOCHS, \n",
    "                 batch_size = BATCH_SIZE,\n",
    "                 # validation_data=(X_test,y_test),\n",
    "                 validation_split = .15,\n",
    "                 class_weight = clas_wts,    # no keras.Softmax() layer!\n",
    "                 # callbacks=[monitor],\n",
    "                 shuffle = True\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_...   Done!   ..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Notes on Using Class Weights**<br>\n",
    "* Final Softmax() layer\n",
    "* Logit vs.Softmax\n",
    "* loss_weights to model.compile()\n",
    "* Using Class Weights in Scikit Learn\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Softmax() layer**\n",
    "<br>If the model really must return a probability,\n",
    "<a href=\"https://www.tensorflow.org/tutorials/quickstart/beginner\"> \n",
    "the tensorflow docs</a> suggest wrapping the *trained* model to attach the softmax to it:<br>\n",
    "> probability_model = tf.keras.Sequential([<br>&emsp;model,<br>&emsp;tf.keras.layers.Softmax()<br>])\n",
    "\n",
    "This throws an error, suggesting it needs to be rewritten in the keras functional model style ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b. Logit vs. Softmax**\n",
    "<br>In statistics, a logistic function is the result of the division of two exponential functions, that gives rise to the logistic curve. Sigmoid refers to various real functions whose graph resembles an elongated letter \"S\"; specifically, the logistic function. The inverse of the sigmoid or logistic function is known as a \"logit\" (logistic unit) function.\n",
    "\n",
    "In machine learning, a logit function (also known as the log-odds function), calculates the natural log of the odds that an observation belongs to one of the classification categories. It yields a vector of K real values for each observation that range from negative infinity to infinity, where K equals the number of classes. \n",
    "\n",
    "The softmax (sigmoid) function transforms a vector of K real values into a vector of K real values that sum to one, which can be interpreted as probabilities. Small or negative inputs become small probabilities, large values become higher probabilities, and the final sum of the probabilities will always be one. \n",
    "\n",
    "Sigmoid is used for binary classification with only 2 classes, while SoftMax applies to multiclass problems - many folks consider sigmoid a special case of softmax. These functions can be used in a classifier only when the classes are mutually exclusive.\n",
    "\n",
    "When from_logits = True, the loss function takes a vector of ground truth values and a vector of logits and returns a scalar loss for each observation. An extra vector of real-valued loss_weights can easily be applied to the logit values for each observation before the final loss is calculated. \n",
    "\n",
    "When from_logits = False (the default in tf.keras) the loss function gets a a vector of ground truth values and a vector of \"softmaxed\" relative probabilities for each class. This \"preprocessing\" makes it impossible to apply loss_weights and get a properly accurate result. \n",
    "\n",
    "In either case, the final loss value is the negative log probability of the true class: the loss is zero if the model is sure of the correct class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loss_weights to model.compile()**\n",
    "<br>Passing the loss_weights to model.compile() works when the final layer of the mofel is keras.Softmax(), which can be used in a classifier only when the classes are mutually exclusive.\n",
    "\n",
    "This functionality is really meant for multilabeled classificatin and models with multiple loss functions [examples exist]. You can assign different levels of importance to the loss values in their contribution to the final loss. \n",
    "\n",
    "One common application is multilabeled classes (not mutually exclusive) like item {shirt shoes, socks} and color {black, white, red} to classify \"Red Shirt\" and \"Black Shoes\", or determine if an image shows a cat, a dog, or both.\n",
    "\n",
    "With multiple loss functions, the book Deep Learning with Python says: \"This is useful in particular if the loss values use different scales. For instance, the mean squared error (MSE) loss used for the age-regression task typically takes a value around 3–5, whereas the cross-entropy loss used for the gender-classification task can be as low as 0.1. In such a situation, to balance the contribution of the different losses, you can assign a weight of 10 to the crossentropy loss and a weight of 0.25 to the MSE loss.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Using Class Weights in Scikit Learn**\n",
    "<br>Scikit Learn has a limited number of classifiers that can take class_weight as an argument:\n",
    "> <br>##--  Linear  --  ##<br>\n",
    "sklearn.linear_model.LogisticRegression<br>\n",
    "sklearn.linear_model.SGDClassifier\n",
    "<br>##  --  Support Vector  --  ##<br>\n",
    "sklearn.svm.SVC<br>\n",
    "sklearn.svm.LinearSVC<br>\n",
    "sklearn.linear_model.RidgeClassifier\n",
    "<br>##  --  Non-linear  --  ##<br>\n",
    "sklearn.tree.DecisionTreeClassifier\n",
    "<br>##  --  Ensemble: bagging  --  ##<br>\n",
    "sklearn.ensemble.RandomForestClassifier<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
