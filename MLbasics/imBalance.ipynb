{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **tomato juice dataset**\n",
    "<br>` 'quality' is the target feature for classification `\n",
    "<br>` the other features are chemical properties of our product `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S34U5S-i69d"
   },
   "source": [
    "**Import the main libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "# supress all\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IZetEZ8jQJm"
   },
   "source": [
    "**Import the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## file path: windows style\n",
    "df = pd.read_csv('..\\\\datasets\\\\tomatjus.csv')\n",
    "\n",
    "## file path: unix style\n",
    "#df = pd.read_csv('../datasets/tomatjus.csv')\n",
    "\n",
    "# shape method gives the dimensions of the dataset\n",
    "print('Dataset dimensions: {} rows, {} columns'.format(\n",
    "    df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Data Preparation and EDA** (unique to this dataset)\n",
    "* _Check for missing values_\n",
    "* _Quick visual check of unique values_\n",
    "* _Split the classification feature out of the dataset_\n",
    "* _Check column names of categorical attributes ( for get_dummies() )_\n",
    "* _Check column names of numeric attributes ( for Scaling )_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "print('Missing Values - ')\n",
    "for col in df.columns:\n",
    "    nnul = pd.notnull(df[col]) \n",
    "    if (len(nnul)!=len(df)):\n",
    "        cnt=cnt+1\n",
    "        print('\\t',col,':',(len(df)-len(nnul)),'null values')\n",
    "print('Total',cnt,'features with null values')\n",
    "\n",
    "# address missing values here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Quick visual check of unique values, deal with unique identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify columns with only one value \n",
    "# or with number of unique values == number of rows\n",
    "n_eq_one = []\n",
    "n_eq_all = []\n",
    "\n",
    "print('Unique value count (',df.shape[0],'Rows in the dataset )')\n",
    "for col in df.columns:\n",
    "    lc = len(df[col].unique())\n",
    "    print(col, ' ::> ', lc)\n",
    "    if lc == 1:\n",
    "        n_eq_one.append(df[col].name)\n",
    "    if lc == df.shape[0]:\n",
    "        n_eq_all.append(df[col].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns with only one value\n",
    "if len(n_eq_one) > 0:\n",
    "    print('Dropping single-valued features')\n",
    "    print(n_eq_one)\n",
    "    data.drop(n_eq_one, axis=1, inplace=True)\n",
    "\n",
    "# Drop or bin columns with number of unique values == number of rows\n",
    "if len(n_eq_all) > 0:\n",
    "    print('Dropping unique identifiers')\n",
    "    print(n_eq_all)\n",
    "    data.drop(n_eq_all, axis=1, inplace=True)\n",
    "\n",
    "# continue with featue selection / feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Classification target feature**\n",
    "<br>\"the Right Answers\", or more formally \"the desired outcome\"\n",
    "<br>Must be in a separate dataset for classification ,,,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Make it a multi-class problem, using text labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  divide into classes by giving a range for quality\n",
    "##  Make it a multi-class problem: {3,4,5} {6} {7.8}\n",
    "bins = (2, 5, 6, 8)\n",
    "group_names = ['Average', 'Premium', 'Special']\n",
    "df['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the classification feature out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature being predicted (\"the Right Answer\")\n",
    "labels_col = 'quality'\n",
    "y = df[labels_col]\n",
    "\n",
    "## Features used for prediction \n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy from slice\n",
    "# so we force it to create a new dataframe \n",
    "X = df.copy()\n",
    "X.drop(labels_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sorted list of unique labels to use later\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "targetlabels = unique_labels(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check column names of categorical attributes**\n",
    "<br>Features with text values (categorical attributes) need to be normalised\n",
    "<br>by changing them to numeric types that the algorithms find easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categori = X.select_dtypes(include=['object','category']).columns\n",
    "print(categori.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check column names of numeric attributes**\n",
    "<br>Features with numeric values need to be normalised\n",
    "<br>by changing them to small numbers in a specific range (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeri = X.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proper place to do scaling comes later in the pipeline ,,, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**<br>Create Test // Train Datasets**\n",
    "> Split X and y datasets into Train and Test subsets,<br>keeping relative proportions of each class (stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=50, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<br>Target Label Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape method gives the dimensions of the dataset\n",
    "print('X_train: {} rows, {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('X_test:  {} rows, {} columns'.format(X_test.shape[0], X_test.shape[1]))\n",
    "print()\n",
    "print('y_train: {} rows, 1 column'.format(y_train.shape[0]))\n",
    "print('y_test:  {} rows, 1 column'.format(y_test.shape[0]))\n",
    "print()\n",
    "\n",
    "## Here's a nice report:  \n",
    "# 1. series to dataframe conversion\n",
    "my_train = pd.DataFrame(y_train)\n",
    "my_test = pd.DataFrame(y_test)\n",
    "# 2. dataframe copy with [[ -- ]]\n",
    "av_train = my_train[[labels_col]].apply(lambda x: x.value_counts())\n",
    "av_test = my_test[[labels_col]].apply(lambda x: x.value_counts())\n",
    "# 3. add a new column\n",
    "av_train['pct_train'] = round((100 * av_train / av_train.sum()),2)\n",
    "av_test['pct_test'] = round((100 * av_test / av_test.sum()),2)\n",
    "# 4. combine the dataframes\n",
    "av_tt = pd.concat([av_train,av_test], axis=1) \n",
    "# 5. print the report\n",
    "print('Frequency and Distribution of labels')\n",
    "print(av_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.target import ClassBalance\n",
    "# The ClassBalance visualizer has a “compare” mode, \n",
    "#   to create a side-by-side bar chart instead of a single bar chart \n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassBalance()\n",
    "visualizer.fit(y_train, y_test)        # Fit the data to the visualizer\n",
    "_ = visualizer.show()                  # Finalize and render the figure\n",
    "# assign visualizer.show() to a null variable to avoid printing some trash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**<br>Handling Imbalance with Over-sampling // Under-sampling**\n",
    "<br>_use only the training data and labels!_\n",
    "<br><br>\n",
    "Sampling with replacement allows duplicate values, so we only do this for the training data, to give it more to work with. The _pandas dataframe.groupby.sample_ method does not allow sample_amounts to be larger than the group size if replace is False, but if replace is True then replacement will occur even in groups that could have been downsampled.\n",
    "<br><br>\n",
    "Here we create a dictionary that maps each class to number of samples, then _groupby.apply_ with a lambda to create the sample, and conditional logic to determine with or without replacement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_col = 'quality'\n",
    "yt = pd.DataFrame(y_train)    # series to dataframe\n",
    "ff = yt[[labels_col]].apply(lambda x: x.value_counts())\n",
    "ss = ff[labels_col].to_dict()\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set new values - anything goes!\n",
    "ss['Premium'] = 450\n",
    "ss['Average'] = ss['Average'] - ss['Special']\n",
    "ss['Special'] = round(ss['Special'] * 2.5)\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the labels back to the features dataframe \n",
    "xy_train = X_train.copy()\n",
    "xy_train[labels_col] = yt[labels_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the index numbers - random order from test_train_split()\n",
    "xy_train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technically, this is a \"one-liner\" ...\n",
    "balanced_df = xy_train.groupby(labels_col, \n",
    "                               as_index=False, group_keys=False, sort=False\n",
    "                        ).apply(lambda g: g.sample(n=ss[g.name],\n",
    "                                                   replace=(len(g) < ss[g.name])\n",
    "                                                  )).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reset the index at the end, so it is neat\n",
    "balanced_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature being predicted (\"the Right Answer\")\n",
    "ytrain_b = balanced_df[labels_col]\n",
    "\n",
    "## Features used for prediction \n",
    "Xtrain_b = balanced_df.drop(labels_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainOriginal = X_train\n",
    "ytrainOriginal = y_train\n",
    "\n",
    "X_train = Xtrain_b\n",
    "y_train = ytrain_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**_This is copied in from above_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<br>Target Label Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape method gives the dimensions of the dataset\n",
    "print('X_train: {} rows, {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('X_test:  {} rows, {} columns'.format(X_test.shape[0], X_test.shape[1]))\n",
    "print()\n",
    "print('y_train: {} rows, 1 column'.format(y_train.shape[0]))\n",
    "print('y_test:  {} rows, 1 column'.format(y_test.shape[0]))\n",
    "print()\n",
    "\n",
    "## Here's a nice report:  \n",
    "# 1. series to dataframe conversion\n",
    "my_train = pd.DataFrame(y_train)\n",
    "my_test = pd.DataFrame(y_test)\n",
    "# 2. dataframe copy with [[ -- ]]\n",
    "av_train = my_train[[labels_col]].apply(lambda x: x.value_counts())\n",
    "av_test = my_test[[labels_col]].apply(lambda x: x.value_counts())\n",
    "# 3. add a new column\n",
    "av_train['pct_train'] = round((100 * av_train / av_train.sum()),2)\n",
    "av_test['pct_test'] = round((100 * av_test / av_test.sum()),2)\n",
    "# 4. combine the dataframes\n",
    "av_tt = pd.concat([av_train,av_test], axis=1) \n",
    "# 5. print the report\n",
    "print('Frequency and Distribution of labels')\n",
    "print(av_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.target import ClassBalance\n",
    "# The ClassBalance visualizer has a “compare” mode, \n",
    "#   to create a side-by-side bar chart instead of a single bar chart \n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassBalance()\n",
    "visualizer.fit(y_train, y_test)        # Fit the data to the visualizer\n",
    "_ = visualizer.show()                  # Finalize and render the figure\n",
    "# assign visualizer.show() to a null variable to avoid printing some trash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**_Copy these standard blocks in from another notebook_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Function** to calculate perfomance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Classifier Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Fit and Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Handling Imbalance with Class Weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Balanced weighting a widely used method for imbalanced classification models. It involves applying specified class weights for the majority and minority classes that are used in the classifier training process to achieve better model results.\n",
    "\n",
    "Unlike over- or under-sampling (a pre-processing step), balanced weighting does not modify the dataset. Instead, each observation is weighted so that wrong predictions for the minority class are given more weight when the loss value is calculated during the model training process. Weights for the loss function can be arbitrary, but a typical choice is class weights (distribution of labels). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**SKlearn Classifiers with Class Weights**\n",
    "<br>A limited number of classifiers can take _class_weight='balanced'_ as an argument. This uses the values of y (labels) to automatically adjust weights inversely proportional to class frequencies in the input data (X) as<br>_n_samples / (n_classes * np.bincount(y))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare list - these support a class_weight argument\n",
    "models = []\n",
    "\n",
    "##  --  Linear  --  ## \n",
    "#from sklearn.linear_model import LogisticRegression \n",
    "#models.append ((\"LogReg\",LogisticRegression(class_weight='balanced'))) \n",
    "#from sklearn.linear_model import SGDClassifier \n",
    "#models.append ((\"StocGradDes\",SGDClassifier(class_weight='balanced'))) \n",
    "\n",
    "##  --  Support Vector  --  ## \n",
    "#from sklearn.svm import SVC \n",
    "#models.append((\"SupportVectorClf\", SVC(class_weight='balanced'))) \n",
    "#from sklearn.svm import LinearSVC \n",
    "#models.append((\"LinearSVC\", LinearSVC(class_weight='balanced'))) \n",
    "#from sklearn.linear_model import RidgeClassifier\n",
    "#models.append ((\"RidgeClf\",RidgeClassifier(class_weight='balanced'))) \n",
    "\n",
    "##  --  Non-linear  --  ## \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "models.append ((\"DecisionTree\",DecisionTreeClassifier(class_weight='balanced'))) \n",
    "\n",
    "##  --  Ensemble: bagging  --  ## \n",
    "#from sklearn.ensemble import RandomForestClassifier \n",
    "#models.append((\"RandomForest\", RandomForestClassifier(class_weight='balanced'))) \n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**_Make sure we are using the right dataset !!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = XtrainOriginal\n",
    "y_train = ytrainOriginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**_Copy the standard block from above_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Fit and Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
