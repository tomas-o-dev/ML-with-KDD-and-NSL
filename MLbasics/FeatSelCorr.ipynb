{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bank \"churn\" dataset**\n",
    "<br>` 'Exited' is our classification target `\n",
    "<br>` 1 - went elsewhere (nonzero is True) `\n",
    "<br>` 0 - remains as a customer `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S34U5S-i69d"
   },
   "source": [
    "**Importing the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IZetEZ8jQJm"
   },
   "source": [
    "**Importing the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## file path: windows style\n",
    "data = pd.read_csv('..\\\\datasets\\\\churn_modelling.csv')\n",
    "\n",
    "## file path: unix style\n",
    "#data = pd.read_csv('../datasets/churn_modelling.csv')\n",
    "\n",
    "# shape method gives the dimensions of the dataset\n",
    "print('Dataset dimensions: {} rows, {} columns'.format(data.shape[0], data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Data Preparation and EDA** (unique to this dataset)\n",
    "* _Check for missing values_\n",
    "* _Quick visual check of unique values_\n",
    "* _Split the classification feature out of the dataset_\n",
    "* _Check column names of categorical attributes ( for get_dummies() )_\n",
    "* _Check column names of numeric attributes ( for Scaling )_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "print('Missing Values - ')\n",
    "for col in data.columns:\n",
    "    nnul = pd.notnull(data[col]) \n",
    "    if (len(nnul)!=len(data)):\n",
    "        cnt=cnt+1\n",
    "        print('\\t',col,':',(len(data)-len(nnul)),'null values')\n",
    "print('Total',cnt,'features with null values')\n",
    "\n",
    "# address missing values here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Quick visual check of unique values, deal with unique identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify columns with only one value \n",
    "# or with number of unique values == number of rows\n",
    "n_eq_one = []\n",
    "n_eq_all = []\n",
    "\n",
    "print('Unique value count (',data.shape[0],'Rows in the dataset )')\n",
    "for col in data.columns:\n",
    "    lc = len(data[col].unique())\n",
    "    print(col, ' ::> ', lc)\n",
    "    if lc == 1:\n",
    "        n_eq_one.append(data[col].name)\n",
    "    if lc == data.shape[0]:\n",
    "        n_eq_all.append(data[col].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns with only one value\n",
    "if len(n_eq_one) > 0:\n",
    "    print('Dropping single-valued features')\n",
    "    print(n_eq_one)\n",
    "    data.drop(n_eq_one, axis=1, inplace=True)\n",
    "\n",
    "# Drop or bin columns with number of unique values == number of rows\n",
    "if len(n_eq_all) > 0:\n",
    "    print('Dropping unique identifiers')\n",
    "    print(n_eq_all)\n",
    "    data.drop(n_eq_all, axis=1, inplace=True)\n",
    "\n",
    "# continue with featue selection / feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's \"bin\" the EstimatedSalary and the Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated Salary - minValue: ',data['EstimatedSalary'].min(),\n",
    "      '  maxValue: ',data['EstimatedSalary'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Balance - minValue: ',data['Balance'].min(),\n",
    "      '  maxValue: ',data['Balance'].max())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the basics: \n",
    "use () and [] to denote how the bin edges are defined:\n",
    "In \"Interval Notation\" we just write the beginning and ending numbers of the interval, with\n",
    "    [ ] a square bracket when we want to include the end value, or\n",
    "    ( ) a round bracket when we don't\n",
    "so  (5, 12] means include values where v > 5 and v < 13 (do not include 5, but do include 12)\n",
    "Note: Bin labels must be one fewer than the number of bin edges\n",
    "      Values that do not fit a bin will be NaN\n",
    "for a lot of gnarly details see https://pbpython.com/pandas-qcut-cut.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_labels = ['(Zero)','Below 1k','1k-35k','36k-59k','60k-95k','96k-119k','120k-179k','180k-239k','240k-300k']\n",
    "cut_bins = [-1, 0, 999, 35999, 59999, 95999, 119999, 179999, 239999, 299999]\n",
    "data['SalaryRange'] = pd.cut(data['EstimatedSalary'], bins=cut_bins, labels=range_labels)\n",
    "data['BalanceRange'] = pd.cut(data['Balance'], bins=cut_bins, labels=range_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique value count: Estimated Salary ',len(data['EstimatedSalary'].unique()),\n",
    "      '  SalaryRange ',len(data['SalaryRange'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique value count: Balance ',len(data['Balance'].unique()),\n",
    "      '  BalanceRange ',len(data['BalanceRange'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the detail and keep the categories\n",
    "#   Using inPlace makes permanent changes to the dataframe in memory \n",
    "#   otherwise drop() will not affect the dataset we are working on\n",
    "data.drop(['EstimatedSalary'], axis=1, inplace=True)\n",
    "data.drop(['Balance'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove one more column that will not help predict the outcome\n",
    "data.drop(['Surname'], axis=1, inplace=True)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Classification target feature**\n",
    "<br>\"the Right Answers\", or more formally \"the desired outcome\"\n",
    "<br>Must be in a separate dataset for classification ,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 'Exited' is our classification target \n",
    "## 1 (nonzero is True) - went elsewhere, zero - remains as a customer\n",
    "print(data['Exited'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Text labels look better in the confusion matrix\n",
    "\n",
    "## a 'lambda' function is always simple, used only once\n",
    "#data.Exited = data.Exited.apply(lambda x: 'Gone' if x==1 else 'Here')\n",
    "\n",
    "## an alternative to a 'lambda' that has the same effect\n",
    "data['Exited'] = ['Gone' if x==1 else 'Here' for x in data['Exited']]\n",
    "\n",
    "## Let's change the name to 'Status' too - 'rename' is like 'drop'\n",
    "## setting the parameter 'inplace' to True changes the original DataFrame \n",
    "## if not set, a new DataFrame is returned\n",
    "data.rename(columns={'Exited': 'Status'}, inplace = True)\n",
    "\n",
    "data['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the classification feature out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Feature being predicted (\"the Right Answer\")\n",
    "labels_col = 'Status'\n",
    "y = data[labels_col]\n",
    "\n",
    "## Features used for prediction \n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy from slice\n",
    "# so we force it to create a new dataframe \n",
    "X = data.copy()\n",
    "X.drop(labels_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sorted list of unique labels to use later\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "targetlabels = unique_labels(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Check column names of categorical attributes**\n",
    "<br>Features with text values (categorical attributes) need to be normalised\n",
    "<br>by changing them to numeric types that the algorithms find easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categori = X.select_dtypes(include=['object','category']).columns\n",
    "print(categori.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the distribution of the feature values \n",
    "for col in categori:\n",
    "    print('Distribution of categories in', col)\n",
    "    print(X[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 'one hot' encoding transforms a single column of text values into \n",
    "multiple columns of discrete values: \n",
    "it creates a new column for each unique value and puts\n",
    "(one) in the column for which it is true and (zero) in the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Country = pd.get_dummies(X.Geography)\n",
    "Country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X, Country], axis=1)\n",
    "X.drop('Geography', axis=1, inplace=True)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the automatic way adds the original feature name\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop one-hot columns with no values (no data in this category)\n",
    "onehot = X.select_dtypes(include=['uint8']).columns\n",
    "for col in onehot:\n",
    "    lc = len(X[col].unique())\n",
    "    if lc == 1:\n",
    "        print('Dropping ',col, ' ::> ', lc)\n",
    "        X.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check column names of numeric attributes**\n",
    "<br>Features with numeric values need to be normalised\n",
    "<br>by changing them to small numbers in a specific range (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeri = X.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proper place to do scaling comes later in the pipeline ,,, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "**<br>Create Test // Train Datasets**\n",
    "> Split X and y datasets into Train and Test subsets,<br>keeping relative proportions of each class (stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=50,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<br>Target Label Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape method gives the dimensions of the dataset\n",
    "print('X_train: {} rows, {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('X_test:  {} rows, {} columns'.format(X_test.shape[0], X_test.shape[1]))\n",
    "print()\n",
    "print('y_train: {} rows, 1 column'.format(y_train.shape[0]))\n",
    "print('y_test:  {} rows, 1 column'.format(y_test.shape[0]))\n",
    "print()\n",
    "\n",
    "## Here's a nice report:  \n",
    "# 1. series to dataframe conversion\n",
    "my_train = pd.DataFrame(y_train)\n",
    "my_test = pd.DataFrame(y_test)\n",
    "# 2. dataframe copy with [[ -- ]]\n",
    "av_train = my_train[[labels_col]].apply(lambda x: x.value_counts())\n",
    "av_test = my_test[[labels_col]].apply(lambda x: x.value_counts())\n",
    "# 3. add a new column\n",
    "av_train['pct_train'] = round((100 * av_train / av_train.sum()),2)\n",
    "av_test['pct_test'] = round((100 * av_test / av_test.sum()),2)\n",
    "# 4. combine the dataframes\n",
    "av_tt = pd.concat([av_train,av_test], axis=1) \n",
    "# 5. print the report\n",
    "print('Frequency and Distribution of labels')\n",
    "print(av_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Next are standard steps for all datasets: _scaling, classifiers, results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**Scaling** comes _after_ test // train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from above\n",
    "# numeri = X.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data before normalization\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the Numeric columns \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# StandardScaler range: -1 to 1, MinMaxScaler range: zero to 1\n",
    "# ColumnTransformer returns a numpy.ndarray so we lose the feature names;\n",
    "# we process one column at a time to preserve the dataframe\n",
    "\n",
    "# sklearn docs say \n",
    "#   \"Don't cheat - fit only on training data, then transform both\"\n",
    "#   fit() expects 2D array: reshape(-1, 1) for single col or (1, -1) single row\n",
    "\n",
    "for i in numeri:\n",
    "    arr = np.array(X_train[i])\n",
    "    scale = MinMaxScaler().fit(arr.reshape(-1, 1))\n",
    "    X_train[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "\n",
    "    arr = np.array(X_test[i])\n",
    "    X_test[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data after normalization\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "**<br>Checking Correlations**<br>\n",
    "_using only the training data and labels_<br> \n",
    "_X and y datasets (complete, not normalised) could be used..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# works best with numeric values for the target feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "## Feature being predicted (\"the Right Answer\")\n",
    "ynum = LabelEncoder().fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pearson Correlation\n",
    "# pandas df.corr() gives different results than yellowbrick! \n",
    "#        so we do this directly with numpy\n",
    "\n",
    "nf = 5        # (number of features to select)\n",
    "\n",
    "# use numpy directly\n",
    "cormx = []\n",
    "for col in X_train.columns:\n",
    "    vals = X_train[col].values\n",
    "    coco = np.corrcoef(vals, ynum)[0,1]\n",
    "    cormx.append((col, coco))\n",
    "\n",
    "# convert to dataframe, select highest and lowest, Join\n",
    "cordf = pd.DataFrame(cormx, columns=['Name','Score'])\n",
    "\n",
    "corhi = cordf.sort_values('Score',ascending=False).head((nf))\n",
    "corlo = cordf.sort_values('Score',ascending=False).tail(nf)\n",
    "\n",
    "corhl = pd.concat([corhi, corlo])\n",
    "\n",
    "corcols = corhl['Name'].values\n",
    "\n",
    "print('Pearson correlation with classification target')\n",
    "print(corhl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = \"Score\", y = \"Name\", data = corhl).set(\n",
    "    title='Pearson Correlation with Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<br>Mutual Info Classification**\n",
    "<br>Calculates a mutual information (MI) value for each independent variable with respect to the dependent variable, and selects the ones with the highest information gain. In other words, basically it tells us how useful the feature X is at predicting the random variable Y on a scale of zero to one, with higher numbers indicating better predictors.<br><br>\n",
    "MI has a lot of advocates because it can capture may types of dependencies. There is a nice illustration of this in the sklearn documentation, comparing MI to the (anova) F-test, which captures only linear dependency like the pearson r metric:<br>\n",
    "https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = 10        # (number of features to select)\n",
    "\n",
    "# This takes a bit longer to run than the Pearson correlations ...\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# add random_state= for consistent output from multiple runs\n",
    "minf = mutual_info_classif(X_train, y_train, random_state = 111)\n",
    "\n",
    "# put the output into a dataframe\n",
    "midf = pd.DataFrame({'Name': X_train.columns, 'Score': minf})\n",
    "\n",
    "# extract the top nf\n",
    "mihi = midf.sort_values('Score', ascending=False).head(nf)\n",
    "\n",
    "micols = mihi['Name'].values\n",
    "\n",
    "print('Mutual Information with classification target')\n",
    "print(mihi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick sns.barplot\n",
    "sns.barplot(x = \"Score\", y = \"Name\", data = mihi).set(\n",
    "    title='Mutual Info Correlation with Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's merge the lists and ...\n",
    "merged = list(corcols)\n",
    "merged.extend(n for n in micols if n not in merged)\n",
    "\n",
    "# ... create a new dataframe with just those columns\n",
    "mX = X_train.filter(merged)\n",
    "\n",
    "mX.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- _Pairwise Correlation of features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rank2D performs pairwise comparisons of each feature in the data set \n",
    "# with a specific metric or algorithm (default: Pearson correlation) \n",
    "# then returns them ranked as a lower left triangle diagram.\n",
    "\n",
    "from yellowbrick.features.rankd import Rank2D\n",
    "\n",
    "visualizer = Rank2D()\n",
    "visualizer.fit(mX, y_train)\n",
    "visualizer.transform(mX)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Correlation Matrix \"HeatMap\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the full heatmap with values\n",
    "from matplotlib.colors import ListedColormap\n",
    "# Generate Color Map\n",
    "colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# from yellowbrick.Rank2D\n",
    "hm=visualizer.ranks_\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(hm, annot=True, cmap=colormap, xticklabels=merged, yticklabels=merged)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create new dataframes with just those columns\n",
    "mXtrain = X_train.filter(merged)\n",
    "mXtest = X_test.filter(merged)\n",
    "\n",
    "mXtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainOriginal = X_train\n",
    "XtestOriginal = X_test\n",
    "\n",
    "X_train = mXtrain\n",
    "X_test = mXtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of the feature names\n",
    "cols = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Fit and Predict**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
