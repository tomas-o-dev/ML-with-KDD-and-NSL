{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **tomato juice dataset**\n",
    "<br>` 'quality' is the target feature for classification `\n",
    "<br>` the other features are chemical properties of our product `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S34U5S-i69d"
   },
   "source": [
    "**Import the main libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "# supress all\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IZetEZ8jQJm"
   },
   "source": [
    "**Import the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## file path: windows style\n",
    "df = pd.read_csv('..\\\\datasets\\\\tomatjus.csv')\n",
    "\n",
    "## file path: unix style\n",
    "#df = pd.read_csv('../datasets/tomatjus.csv')\n",
    "\n",
    "# shape method gives the dimensions of the dataset\n",
    "print('Dataset dimensions: {} rows, {} columns'.format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Data Preparation and EDA** (unique to this dataset)\n",
    "* _Check for missing values_\n",
    "* _Quick visual check of unique values_\n",
    "* _Split the classification feature out of the dataset_\n",
    "* _Check column names of categorical attributes ( for get_dummies() )_\n",
    "* _Check column names of numeric attributes ( for Scaling )_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Let's skip the checking_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification target feature**\n",
    "<br>_Make it a multi-class problem, using text labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##  divide into classes by giving a range for quality\n",
    "##  Make it a multi-class problem: {3,4,5} {6} {7.8}\n",
    "bins = (2, 5, 6, 8)\n",
    "group_names = ['Average', 'Premium', 'Special']\n",
    "df['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the classification feature out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Feature being predicted (\"the Right Answer\")\n",
    "labels_col = 'quality'\n",
    "y = df[labels_col]\n",
    "\n",
    "## Features used for prediction \n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy from slice\n",
    "# so we force it to create a new dataframe \n",
    "X = df.copy()\n",
    "X.drop(labels_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sorted list of unique labels to use later\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "targetlabels = unique_labels(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**<br>Create Test // Train Datasets**\n",
    "> Split X and y datasets into Train and Test subsets,<br>keeping relative proportions of each class (stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=50, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<br>Target Label Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# shape method gives the dimensions of the dataset\n",
    "print('X_train: {} rows, {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('X_test:  {} rows, {} columns'.format(X_test.shape[0], X_test.shape[1]))\n",
    "print()\n",
    "print('y_train: {} rows, 1 column'.format(y_train.shape[0]))\n",
    "print('y_test:  {} rows, 1 column'.format(y_test.shape[0]))\n",
    "print()\n",
    "\n",
    "## Here's a nice report:  \n",
    "# 1. series to dataframe conversion\n",
    "my_train = pd.DataFrame(y_train)\n",
    "my_test = pd.DataFrame(y_test)\n",
    "# 2. dataframe copy with [[ -- ]]\n",
    "av_train = my_train[[labels_col]].apply(lambda x: x.value_counts())\n",
    "av_test = my_test[[labels_col]].apply(lambda x: x.value_counts())\n",
    "# 3. add a new column\n",
    "av_train['pct_train'] = round((100 * av_train / av_train.sum()),2)\n",
    "av_test['pct_test'] = round((100 * av_test / av_test.sum()),2)\n",
    "# 4. combine the dataframes\n",
    "av_tt = pd.concat([av_train,av_test], axis=1) \n",
    "# 5. print the report\n",
    "print('Frequency and Distribution of labels')\n",
    "print(av_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Next are standard steps for all datasets: _scaling, classifiers, results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check column names of numeric attributes**\n",
    "<br>Features with numeric values need to be normalised\n",
    "<br>by changing them to small numbers in a specific range (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeri = X.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**Scaling** comes _after_ test // train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the Numeric columns \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# StandardScaler range: -1 to 1, MinMaxScaler range: zero to 1\n",
    "# ColumnTransformer returns a numpy.ndarray so we lose the feature names;\n",
    "# we process one column at a time to preserve the dataframe\n",
    "\n",
    "# sklearn docs say \n",
    "#   \"Don't cheat - fit only on training data, then transform both\"\n",
    "#   fit() expects 2D array: reshape(-1, 1) for single col or (1, -1) single row\n",
    "\n",
    "for i in numeri:\n",
    "    arr = np.array(X_train[i])\n",
    "    scale = MinMaxScaler().fit(arr.reshape(-1, 1))\n",
    "    X_train[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "\n",
    "    arr = np.array(X_test[i])\n",
    "    X_test[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Imports** for perfomance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Function** to calculate perfomance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_metrics(y_test,ygx,lbls):\n",
    "    tptn_df = pd.DataFrame(confusion_matrix(y_test, ygx, labels=lbls), \n",
    "                           index=['train:{:}'.format(x) for x in lbls], \n",
    "                           columns=['pred:{:}'.format(x) for x in lbls])\n",
    "    print(tptn_df)    \n",
    "    print(\"\\n~~~~\")\n",
    "    \n",
    "    TP = np.diag(tptn_df.values)\n",
    "    FP = tptn_df.values.sum(axis=0) - TP\n",
    "    FN = tptn_df.values.sum(axis=1) - TP\n",
    "    TN = np.sum(tptn_df.values) - (FP + FN + TP)\n",
    "# false positive rates\n",
    "    FPR = FP/(FP+TN)\n",
    "# false negative rates\n",
    "    FNR = FN/(TP+FN)\n",
    "# overall \n",
    "    sfpr=FP.sum()/(FP.sum()+TN.sum())\n",
    "    sfnr=FN.sum()/(TP.sum()+FN.sum())\n",
    "    \n",
    "    if len(lbls) >2:\n",
    "        for x in range(len(lbls)):\n",
    "            print('{:>12} : '.format(lbls[x]),\n",
    "                  'FPR = %.3f   FNR = %.3f' % (FPR[x], FNR[x]))\n",
    "        print()\n",
    "\n",
    "    print('{:>12} : '.format('macro avg'),\n",
    "          'FPR = %.3f   FNR = %.3f'  % (FPR.mean(), FNR.mean()))\n",
    "    print('weighted avg :  FPR = %.3f   FNR = %.3f' % (sfpr, sfnr))\n",
    " \n",
    "    print(\"\\n~~~~\")\n",
    "    \n",
    "#    macro average: unweighted mean per label \n",
    "# weighted average: support-weighted mean per label  \n",
    "    print(classification_report(y_test, ygx, digits=3, target_names=lbls))\n",
    "\n",
    "    print(\"~~~~\")\n",
    "# Matthews correlation coefficient: \n",
    "#   correlation between prediction and ground truth\n",
    "#   (+1 perfect, 0 random prediction, -1 inverse)\n",
    "\n",
    "    mcc = matthews_corrcoef(y_test, ygx)\n",
    "    print('MCC: Overall :  %.3f' % mcc)\n",
    "    if len(lbls) >2:\n",
    "        for tc in lbls:\n",
    "            bin_mcc = matthews_corrcoef(y_test == tc, ygx == tc)\n",
    "            print('{:>12} :'.format(tc),' %.3f' % bin_mcc)  \n",
    "\n",
    "    return '~~~~'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "**Hyperparameter Tuning**\n",
    "> General pattern:<br>\n",
    "    1. Classifier selection<br> \n",
    "    2. Fit and Predict<br>\n",
    "    3. Bias-Variance Tradeoff<br>\n",
    "    4. Select strategy and hyperparameters<br>\n",
    "    5. Plug in the best parameter values<br>\n",
    "    6. Fit and Predict<br>\n",
    "    7. Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***\n",
    " **_These examples only work with one classifier_**\n",
    " ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier Selection - Only One!\n",
    "models = []\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "models.append((\"K-NNeighbors\", KNeighborsClassifier())) \n",
    "print(models[0][0])\n",
    "print(models[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Fit and Predict** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "for name, clf in models:\n",
    "    print('Confusion Matrix:', name)\n",
    "    clf.fit(X_train, y_train)\n",
    "    ygx = clf.predict(X_test)\n",
    "\n",
    "    results.append((name, ygx))\n",
    "    \n",
    "# Easy way to ensure that the confusion matrix rows and columns\n",
    "#   are labeled exactly as the classifier has coded the classes\n",
    "#   [[note the _ at the end of clf.classes_ ]]\n",
    "    show_metrics(y_test,ygx,clf.classes_)\n",
    "    print('\\nParameters: ', clf.get_params(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Imports** for Bias - Variance Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "## bias_variance_decomp() requires \n",
    "##    1. numpy ndarrays\n",
    "##    2. numeric targets\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Function** to calculate Bias - Variance Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bias_var_metrics(clf,folds=200):\n",
    "# slow because it does num_rounds (default=200) bootstrap cross validation\n",
    "\n",
    "# numeric targets\n",
    "    ytrain = LabelEncoder().fit_transform(y_train)\n",
    "    ytest = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "    print('Bias // Variance Decomposition:', clf)\n",
    "    avg_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        clf, X_train.values, ytrain, X_test.values, ytest, \n",
    "        loss='0-1_loss', num_rounds=folds, random_seed=44)\n",
    "    print('   Average bias: %.3f' % avg_bias)\n",
    "    print('   Average variance: %.3f' % avg_var)\n",
    "    print('   Average expected loss: %.3f  \"Goodness\": %.3f' % (avg_loss, (1-avg_loss)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Bias - Variance Decomposition** (standard block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bias_var_metrics(clf=models[0][1], folds=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Imports** for parameter testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Default scorer for classification is sklearn.metrics.accuracy_score \n",
    "# In unbalanced classification, the accuracy score is often uninformative\n",
    "# For the list of options see\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "\n",
    "# average for each label weighted by support (number of true instances for each label)\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#cross-validation ALWAYS takes a long time\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Parameter Testing**\n",
    ">Select this block - Go to the Run menu - Run all Above\n",
    "<br>Then pick a strategy and run the blocks one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Strategy: simple loop (no CV), manual selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ---- Specific to each classifier! ---- #\n",
    "clf = models[0][1]\n",
    "\n",
    "# hyperparameter values to test: one constant, one set\n",
    "cname = 'n_neighbors'\n",
    "cval = 3\n",
    "\n",
    "paramname = 'weights'\n",
    "paramrange = ['uniform', 'distance']\n",
    "# ----    ---- #\n",
    "\n",
    "for param in paramrange:\n",
    "    print(models[0][0],':',cname,'=',cval,' ',paramname,'=',param)\n",
    "\n",
    "# use a dict to set multiple parameters\n",
    "    pdict = {cname:cval, paramname:param}\n",
    "    clfp = clf.set_params(**pdict)\n",
    "\n",
    "# fit and predict with the new values\n",
    "    clfp.fit(X_train,y_train)\n",
    "    pred = clfp.predict(X_test)\n",
    "    lbls = clfp.classes_\n",
    "\n",
    "    waa = balanced_accuracy_score(y_test, pred)\n",
    "    print('Weighted Average Accuracy:  %.3f' % waa)\n",
    "    \n",
    "    waf = f1_score(y_test, pred, average='weighted')\n",
    "    print('Weighted Average f1_score:  %.3f' % waf)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "*<br>MANUAL EDITING: Plug in the best parameter values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- put best parameters into the model --##\n",
    "## be sure to use a valid value from the result above !!\n",
    "param = 'distance'\n",
    "# ----    ---- #\n",
    "# use a dict to set multiple parameters\n",
    "best_vals = {cname:cval, paramname:param}\n",
    "models[0][1].set_params(**best_vals)\n",
    "\n",
    "print(models[0][0],': Best Values ')\n",
    "print(models[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Fit and Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste in the standard block from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "for name, clf in models:\n",
    "    print('Confusion Matrix:', name)\n",
    "    clf.fit(X_train, y_train)\n",
    "    ygx = clf.predict(X_test)\n",
    "\n",
    "    results.append((name, ygx))\n",
    "    \n",
    "# Easy way to ensure that the confusion matrix rows and columns\n",
    "#   are labeled exactly as the classifier has coded the classes\n",
    "#   [[note the _ at the end of clf.classes_ ]]\n",
    "    show_metrics(y_test,ygx,clf.classes_)\n",
    "    print('\\nParameters: ', clf.get_params(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Bias - Variance Decomposition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# paste in the standard block from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Strategy: Single Parameter Cross-Validation Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# a variation of KFold that returns stratified folds,  \n",
    "#   preserving the percentage of samples for each class\n",
    "from yellowbrick.model_selection import ValidationCurve\n",
    "\n",
    "# ---- Specific to each classifier! ---- #\n",
    "clf = models[0][1]\n",
    "\n",
    "## hyperparameter (can only graph one)\n",
    "paramname = \"n_neighbors\"\n",
    "# range for the hyperparameter \n",
    "#        start, stop, and step \n",
    "paramrange = np.arange(2, 9, 1)\n",
    "\n",
    "## or, for example:\n",
    "#paramname = \"p\"\n",
    "#paramrange = [1, 2, 4]\n",
    "\n",
    "## or, for example:\n",
    "#paramname = 'weights'\n",
    "#paramrange = ['uniform', 'distance']\n",
    "\n",
    "# ----  Cross Validation  ---- #\n",
    "# number of rounds\n",
    "folds = 3\n",
    "\n",
    "# average for each label weighted by support (number of true instances for each label)\n",
    "scorer = ['wtd.avg.accuracy', 'balanced_accuracy']\n",
    "#scorer = ['wtd.avg.f1_score', 'f1_weighted']\n",
    "# ----    ---- #\n",
    "\n",
    "# start the timer\n",
    "trs = time()\n",
    "\n",
    "print(models[0][0], '\\t(timer started)')\n",
    "print('Validation Curve for parameter [',paramname,'], scoring =',scorer[0])\n",
    "print()\n",
    "\n",
    "skf = StratifiedKFold(shuffle=True, random_state = 11, n_splits=folds)\n",
    "\n",
    "# Create the validation curve visualizer\n",
    "viz = ValidationCurve(\n",
    "    clf, param_name=paramname, param_range=paramrange,\n",
    "    logx=True, cv=skf, scoring=scorer[1], n_jobs= -1)\n",
    "\n",
    "viz.fit(X_train, y_train)\n",
    "viz.show()\n",
    "\n",
    "tre = time() - trs\n",
    "print (\"Run Time {} seconds\".format(round(tre,2)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy & Paste blocks:\n",
    "# 5. MANUAL EDITING: Plug in the best parameter values\n",
    "# 6. Fit and Predict (standard block)\n",
    "# 7. Bias-Variance Tradeoff (standard block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Strategy: Bias - Variance Decomposition: Parameter testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Specific to each classifier! ---- #\n",
    "clf = models[0][1]\n",
    "\n",
    "# hyperparameter values to test: one constant, one set\n",
    "cname = 'n_neighbors'\n",
    "cval = 3\n",
    "\n",
    "paramname = \"p\"\n",
    "paramrange = [1, 2, 4]\n",
    "\n",
    "# ----  Cross Validation  ---- #\n",
    "# adjust num_rounds (default=200) for bootstrap cross validation\n",
    "folds = 3\n",
    "# ----    ---- #\n",
    "\n",
    "# start the timer\n",
    "trs = time()\n",
    "\n",
    "## bias_variance_decomp() requires \n",
    "##    1. numpy ndarrays\n",
    "##    2. numeric targets\n",
    "ytrain = LabelEncoder().fit_transform(y_train)\n",
    "ytest = LabelEncoder().fit_transform(y_test)\n",
    "\n",
    "bias, var, err = [], [], []\n",
    "\n",
    "for parm in paramrange:\n",
    "    print(models[0][0],'(',cname,'=',cval,' ',paramname,'=',parm,')',end='') # no newline at the end\n",
    "\n",
    "# use a dict to set multiple parameters\n",
    "    pdict = {cname:cval, paramname:parm}\n",
    "    clfp = clf.set_params(**pdict)\n",
    "\n",
    "# fit and predict with the new values\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        clf, X_train.values, ytrain, X_test.values, ytest, \n",
    "        loss='0-1_loss', num_rounds=folds, random_seed=11)\n",
    "    err.append(avg_expected_loss)\n",
    "    bias.append(avg_bias)\n",
    "    var.append(avg_var)\n",
    "\n",
    "    msg=\"  Bias: %0.3f  Variance: %0.3f  E.loss: %0.3f\" % (avg_bias, avg_var, avg_expected_loss)\n",
    "    print(msg)\n",
    "\n",
    "tre = time() - trs\n",
    "print (\"\\tRun Time {} seconds\".format(round(tre,2)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(3,17), err, 'b', label = 'total_error')\n",
    "plt.plot(range(3,17), bias, 'k', label = 'bias')\n",
    "plt.plot(range(3,17), var, 'y', label = 'variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy & Paste blocks:\n",
    "# 5. MANUAL EDITING: Plug in the best parameter values\n",
    "# 6. Fit and Predict (standard block)\n",
    "# 7. Bias-Variance Tradeoff (standard block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Strategy: Parameter grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T20:06:08.173060Z",
     "start_time": "2020-04-10T20:05:58.792577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Each parameter increases time exponentially!\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ---- Specific to each classifier! ---- #\n",
    "clf = models[0][1]\n",
    "\n",
    "# hyperparameter: values to test (minimum 2x2 Grid)\n",
    "\n",
    "value_grid = {'n_neighbors': [3, 4], \n",
    "              'p': [1, 2, 4], \n",
    "              'weights': ['uniform', 'distance']}\n",
    "\n",
    "# ----  Cross Validation  ---- #\n",
    "# number of rounds\n",
    "folds = 3\n",
    "\n",
    "# average for each label weighted by support (number of true instances for each label)\n",
    "scorer = ['wtd.avg.accuracy', 'balanced_accuracy']\n",
    "#scorer = ['wtd.avg.f1_score', 'f1_weighted']\n",
    "# ----    ---- #\n",
    "\n",
    "# Start the timer\n",
    "trs = time()\n",
    "\n",
    "print('GridSearchCV:',folds,'folds, timer started')\n",
    "print('%s with scoring = %s' % (clf, scorer[0]))\n",
    "    \n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=value_grid, \n",
    "                           scoring=scorer[1], cv=folds, verbose=1, n_jobs= -1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "## uncomment these to see the details \n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "# Print best parameters and score\n",
    "print(\"\\tBest parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"\\tBest CV score: {:.3f}\".format(grid_search.best_score_))\n",
    "\n",
    "tre = time() - trs\n",
    "print (\"Run Time {} seconds\".format(round(tre,2)) + '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visualisations:\n",
    "    https://sklearn-evaluation.readthedocs.io/en/latest/user_guide/grid_search.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "*<br>Plug in the best parameter values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid_Search returns a dict of best paraeters\n",
    "models[0][1].set_params(**grid_search.best_params_)\n",
    "\n",
    "print(models[0][0],': Best Values ')\n",
    "print(models[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Copy & Paste blocks:\n",
    "# 6. Fit and Predict (standard block)\n",
    "# 7. Bias-Variance Tradeoff (standard block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
