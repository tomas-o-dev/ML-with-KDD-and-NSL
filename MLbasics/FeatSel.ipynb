{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bank \"churn\" dataset**\n",
    "<br>` 'Exited' is our classification target `\n",
    "<br>` 1 - went elsewhere (nonzero is True) `\n",
    "<br>` 0 - remains as a customer `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S34U5S-i69d"
   },
   "source": [
    "**Importing the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IZetEZ8jQJm"
   },
   "source": [
    "**Importing the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## file path: windows style\n",
    "data = pd.read_csv('..\\\\datasets\\\\churn_modelling.csv')\n",
    "\n",
    "## file path: unix style\n",
    "#data = pd.read_csv('../datasets/churn_modelling.csv')\n",
    "\n",
    "# shape method gives the dimensions of the dataset\n",
    "print('Dataset dimensions: {} rows, {} columns'.format(\n",
    "    data.shape[0], data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Data Preparation and EDA** (unique to this dataset)\n",
    "* _Check for missing values_\n",
    "* _Quick visual check of unique values_\n",
    "* _Split the classification feature out of the dataset_\n",
    "* _Check column names of categorical attributes ( for get_dummies() )_\n",
    "* _Check column names of numeric attributes ( for Scaling )_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "print('Missing Values - ')\n",
    "for col in data.columns:\n",
    "    nnul = pd.notnull(data[col]) \n",
    "    if (len(nnul)!=len(data)):\n",
    "        cnt=cnt+1\n",
    "        print('\\t',col,':',(len(data)-len(nnul)),'null values')\n",
    "print('Total',cnt,'features with null values')\n",
    "\n",
    "# address missing values here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Quick visual check of unique values, deal with unique identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify columns with only one value \n",
    "# or with number of unique values == number of rows\n",
    "n_eq_one = []\n",
    "n_eq_all = []\n",
    "\n",
    "print('Unique value count (',data.shape[0],'Rows in the dataset )')\n",
    "for col in data.columns:\n",
    "    lc = len(data[col].unique())\n",
    "    print(col, ' ::> ', lc)\n",
    "    if lc == 1:\n",
    "        n_eq_one.append(data[col].name)\n",
    "    if lc == data.shape[0]:\n",
    "        n_eq_all.append(data[col].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns with only one value\n",
    "if len(n_eq_one) > 0:\n",
    "    print('Dropping single-valued features')\n",
    "    print(n_eq_one)\n",
    "    data.drop(n_eq_one, axis=1, inplace=True)\n",
    "\n",
    "# Drop or bin columns with number of unique values == number of rows\n",
    "if len(n_eq_all) > 0:\n",
    "    print('Dropping unique identifiers')\n",
    "    print(n_eq_all)\n",
    "    data.drop(n_eq_all, axis=1, inplace=True)\n",
    "\n",
    "# continue with featue selection / feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's \"bin\" the EstimatedSalary and the Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated Salary - minValue: ',data['EstimatedSalary'].min(),\n",
    "      '  maxValue: ',data['EstimatedSalary'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Balance - minValue: ',data['Balance'].min(),\n",
    "      '  maxValue: ',data['Balance'].max())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "the basics: \n",
    "use () and [] to denote how the bin edges are defined:\n",
    "In \"Interval Notation\" we just write the beginning and ending numbers of the interval, with\n",
    "    [ ] a square bracket when we want to include the end value, or\n",
    "    ( ) a round bracket when we don't\n",
    "so  (5, 12] means include values where v > 5 and v < 13 (do not include 5, but do include 12)\n",
    "Note: Bin labels must be one fewer than the number of bin edges\n",
    "      Values that do not fit a bin will be NaN\n",
    "for a lot of gnarly details see https://pbpython.com/pandas-qcut-cut.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_labels = ['(Zero)','Below 1k','1k-35k','36k-59k','60k-95k','96k-119k','120k-179k','180k-239k','240k-300k']\n",
    "cut_bins = [-1, 0, 999, 35999, 59999, 95999, 119999, 179999, 239999, 299999]\n",
    "data['SalaryRange'] = pd.cut(data['EstimatedSalary'], bins=cut_bins, labels=range_labels)\n",
    "data['BalanceRange'] = pd.cut(data['Balance'], bins=cut_bins, labels=range_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique value count: Estimated Salary ',len(data['EstimatedSalary'].unique()),\n",
    "      '  SalaryRange ',len(data['SalaryRange'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique value count: Balance ',len(data['Balance'].unique()),\n",
    "      '  BalanceRange ',len(data['BalanceRange'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop the detail and keep the categories\n",
    "#   Using inPlace makes permanent changes to the dataframe in memory \n",
    "#   otherwise drop() will not affect the dataset we are working on\n",
    "data.drop(['EstimatedSalary'], axis=1, inplace=True)\n",
    "data.drop(['Balance'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove one more column that will not help predict the outcome\n",
    "data.drop(['Surname'], axis=1, inplace=True)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Classification target feature**\n",
    "<br>\"the Right Answers\", or more formally \"the desired outcome\"\n",
    "<br>Must be in a separate dataset for classification ,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a binary classification dataset\n",
    "twoclass = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 'Exited' is our classification target \n",
    "## 1 (nonzero is True) - went elsewhere, zero - remains as a customer\n",
    "print(data['Exited'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Text labels look better in the confusion matrix\n",
    "\n",
    "## a 'lambda' function is always simple, used only once\n",
    "#data.Exited = data.Exited.apply(lambda x: 'Gone' if x==1 else 'Here')\n",
    "\n",
    "## an alternative to a 'lambda' that has the same effect\n",
    "data['Exited'] = ['Gone' if x==1 else 'Here' for x in data['Exited']]\n",
    "\n",
    "## Let's change the name to 'Status' too - 'rename' is like 'drop'\n",
    "## setting the parameter 'inplace' to True changes the original DataFrame \n",
    "## if not set, a new DataFrame is returned\n",
    "data.rename(columns={'Exited': 'Status'}, inplace = True)\n",
    "\n",
    "data['Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the classification feature out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Feature being predicted (\"the Right Answer\")\n",
    "labels_col = 'Status'\n",
    "y = data[labels_col]\n",
    "\n",
    "## Features used for prediction \n",
    "X = data.copy()\n",
    "X.drop(labels_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sorted list of unique labels to use later\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "targetlabels = unique_labels(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Check column names of categorical attributes**\n",
    "<br>Features with text values (categorical attributes) need to be normalised\n",
    "<br>by changing them to numeric types that the algorithms find easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categori = X.select_dtypes(include=['object','category']).columns\n",
    "print(categori.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check the distribution of the feature values \n",
    "for col in categori:\n",
    "    print('Distribution of categories in', col)\n",
    "    print(X[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 'one hot' encoding transforms a single column of text values into \n",
    "multiple columns of discrete values: \n",
    "it creates a new column for each unique value and puts\n",
    "(one) in the column for which it is true and (zero) in the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Country = pd.get_dummies(X.Geography)\n",
    "Country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X, Country], axis=1)\n",
    "X.drop('Geography', axis=1, inplace=True)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the automatic way adds the original feature name\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop one-hot columns with no values (no data in this category)\n",
    "onehot = X.select_dtypes(include=['uint8']).columns\n",
    "for col in onehot:\n",
    "    lc = len(X[col].unique())\n",
    "    if lc == 1:\n",
    "        print('Dropping ',col, ' ::> ', lc)\n",
    "        X.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check column names of numeric attributes**\n",
    "<br>Features with numeric values need to be normalised\n",
    "<br>by changing them to small numbers in a specific range (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeri = X.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proper place to do scaling comes later in the pipeline ,,, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "***\n",
    "**<br>Create Test // Train Datasets**\n",
    "> Split X and y datasets into Train and Test subsets,<br>keeping relative proportions of each class (stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=50,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<br>Target Label Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape method gives the dimensions of the dataset\n",
    "print('X_train: {} rows, {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('X_test:  {} rows, {} columns'.format(X_test.shape[0], X_test.shape[1]))\n",
    "print()\n",
    "print('y_train: {} rows, 1 column'.format(y_train.shape[0]))\n",
    "print('y_test:  {} rows, 1 column'.format(y_test.shape[0]))\n",
    "print()\n",
    "\n",
    "## Here's a nice report:  \n",
    "# 1. series to dataframe conversion\n",
    "my_train = pd.DataFrame(y_train)\n",
    "my_test = pd.DataFrame(y_test)\n",
    "# 2. dataframe copy with [[ -- ]]\n",
    "av_train = my_train[[labels_col]].apply(lambda x: x.value_counts())\n",
    "av_test = my_test[[labels_col]].apply(lambda x: x.value_counts())\n",
    "# 3. add a new column\n",
    "av_train['pct_train'] = round((100 * av_train / av_train.sum()),2)\n",
    "av_test['pct_test'] = round((100 * av_test / av_test.sum()),2)\n",
    "# 4. combine the dataframes\n",
    "av_tt = pd.concat([av_train,av_test], axis=1) \n",
    "# 5. print the report\n",
    "print('Frequency and Distribution of labels')\n",
    "print(av_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Next are standard steps for all datasets: _scaling, classifiers, results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**Scaling** comes _after_ test // train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from above\n",
    "# numeri = X.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data before normalization\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the Numeric columns \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# StandardScaler range: -1 to 1, MinMaxScaler range: zero to 1\n",
    "# ColumnTransformer returns a numpy.ndarray so we lose the feature names;\n",
    "# we process one column at a time to preserve the dataframe\n",
    "\n",
    "# sklearn docs say \n",
    "#   \"Don't cheat - fit only on training data, then transform both\"\n",
    "#   fit() expects 2D array: reshape(-1, 1) for single col or (1, -1) single row\n",
    "\n",
    "for i in numeri:\n",
    "    arr = np.array(X_train[i])\n",
    "    scale = MinMaxScaler().fit(arr.reshape(-1, 1))\n",
    "    X_train[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "\n",
    "    arr = np.array(X_test[i])\n",
    "    X_test[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data after normalization\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Function** to calculate perfomance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_metrics(y_test,ygx,lbls):\n",
    "    tptn_df = pd.DataFrame(confusion_matrix(y_test, ygx, labels=lbls), \n",
    "                           index=['train:{:}'.format(x) for x in lbls], \n",
    "                           columns=['pred:{:}'.format(x) for x in lbls])\n",
    "    print(tptn_df)    \n",
    "    print(\"\\n~~~~\")\n",
    "    \n",
    "    TP = np.diag(tptn_df.values)\n",
    "    FP = tptn_df.values.sum(axis=0) - TP\n",
    "    FN = tptn_df.values.sum(axis=1) - TP\n",
    "    TN = np.sum(tptn_df.values) - (FP + FN + TP)\n",
    "# false positive rates\n",
    "    FPR = FP/(FP+TN)\n",
    "# false negative rates\n",
    "    FNR = FN/(TP+FN)\n",
    "# overall \n",
    "    sfpr=FP.sum()/(FP.sum()+TN.sum())\n",
    "    sfnr=FN.sum()/(TP.sum()+FN.sum())\n",
    "    \n",
    "    if len(lbls) >2:\n",
    "        for x in range(len(lbls)):\n",
    "            print('{:>12} : '.format(lbls[x]),\n",
    "                  'FPR = %.3f   FNR = %.3f' % (FPR[x], FNR[x]))\n",
    "        print()\n",
    "\n",
    "    print('{:>12} : '.format('macro avg'),\n",
    "          'FPR = %.3f   FNR = %.3f'  % (FPR.mean(), FNR.mean()))\n",
    "    print('weighted avg :  FPR = %.3f   FNR = %.3f' % (sfpr, sfnr))\n",
    " \n",
    "    print(\"\\n~~~~\")\n",
    "    \n",
    "#    macro average: unweighted mean per label \n",
    "# weighted average: support-weighted mean per label  \n",
    "    print(classification_report(y_test, ygx, digits=3, target_names=lbls))\n",
    "\n",
    "    print(\"~~~~\")\n",
    "# Matthews correlation coefficient: \n",
    "#   correlation between prediction and ground truth\n",
    "#   (+1 perfect, 0 random prediction, -1 inverse)\n",
    "\n",
    "    mcc = matthews_corrcoef(y_test, ygx)\n",
    "    print('MCC: Overall :  %.3f' % mcc)\n",
    "    if len(lbls) >2:\n",
    "        for tc in lbls:\n",
    "            bin_mcc = matthews_corrcoef(y_test == tc, ygx == tc)\n",
    "            print('{:>12} :'.format(tc),' %.3f' % bin_mcc)  \n",
    "\n",
    "    return '~~~~'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Classifier Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare list\n",
    "models = []\n",
    "\n",
    "##  --  Linear  --  ## \n",
    "#from sklearn.linear_model import LogisticRegression \n",
    "#models.append ((\"LogReg\",LogisticRegression())) \n",
    "#from sklearn.linear_model import SGDClassifier \n",
    "#models.append ((\"StocGradDes\",SGDClassifier())) \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis \n",
    "models.append((\"LinearDA\", LinearDiscriminantAnalysis())) \n",
    "#from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \n",
    "#models.append((\"QuadraticDA\", QuadraticDiscriminantAnalysis())) \n",
    "\n",
    "##  --  Support Vector  --  ## \n",
    "#from sklearn.svm import SVC \n",
    "#models.append((\"SupportVectorClf\", SVC())) \n",
    "#from sklearn.svm import LinearSVC \n",
    "#models.append((\"LinearSVC\", LinearSVC())) \n",
    "#from sklearn.linear_model import RidgeClassifier\n",
    "#models.append ((\"RidgeClf\",RidgeClassifier())) \n",
    "\n",
    "##  --  Non-linear  --  ## \n",
    "#from sklearn.tree import DecisionTreeClassifier \n",
    "#models.append ((\"DecisionTree\",DecisionTreeClassifier())) \n",
    "#from sklearn.naive_bayes import GaussianNB \n",
    "#models.append ((\"GaussianNB\",GaussianNB())) \n",
    "#from sklearn.neighbors import KNeighborsClassifier \n",
    "#models.append((\"K-NNeighbors\", KNeighborsClassifier())) \n",
    "\n",
    "##  --  Ensemble: bagging  --  ## \n",
    "#from sklearn.ensemble import RandomForestClassifier \n",
    "#models.append((\"RandomForest\", RandomForestClassifier())) \n",
    "##  --  Ensemble: boosting  --  ## \n",
    "#from sklearn.ensemble import AdaBoostClassifier \n",
    "#models.append((\"AdaBoost\", AdaBoostClassifier())) \n",
    "#from sklearn.ensemble import GradientBoostingClassifier \n",
    "#models.append((\"GradientBoost\", GradientBoostingClassifier())) \n",
    "\n",
    "##  --  NeuralNet (simplest)  --  ## \n",
    "#from sklearn.linear_model import Perceptron \n",
    "#models.append ((\"SingleLayerPtron\",Perceptron())) \n",
    "#from sklearn.neural_network import MLPClassifier \n",
    "#models.append((\"MultiLayerPtron\", MLPClassifier()))\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Fit and Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "for name, clf in models:\n",
    "    print('Confusion Matrix:', name)\n",
    "    clf.fit(X_train, y_train)\n",
    "    ygx = clf.predict(X_test)\n",
    "\n",
    "    results.append((name, ygx))\n",
    "    \n",
    "# Easy way to ensure that the confusion matrix rows and columns\n",
    "#   are labeled exactly as the classifier has coded the classes\n",
    "#   [[note the _ at the end of clf.classes_ ]]\n",
    "    show_metrics(y_test,ygx,clf.classes_)\n",
    "    print('\\nParameters: ', clf.get_params(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***\n",
    " **_These examples only work with one classifier_**\n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**<br>Feature Importance Permutation**\n",
    "<br>This can be used with any classifier or regressor to estimate feature importance.\n",
    "<br>\n",
    "It returns two arrays: the first array (here: `imp_vals`) contains the actual \n",
    "   importance values we are interested in. The second array is assigned to ` _ ` because we are not using it.\n",
    "When `num_rounds` > 1 the permutation is repeated multiple times \n",
    "   with different random seeds, and the first array holds \n",
    "   the average of the importance values,\n",
    "   with all individual values from these runs in the second array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of the feature names\n",
    "cols = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# works best with numeric values for the target feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "## Feature being predicted (\"the Right Answer\")\n",
    "ynum = LabelEncoder().fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note: for roc_auc_score\n",
    "# we need to predict the probabilities, \n",
    "#     y_prob = clf.predict_proba(X_test)\n",
    "# instead of predicting the class (like above)\n",
    "#     y_pred = clf.predict(X_test)\n",
    "#\n",
    "# multiclass (ovr) works fine with\n",
    "#     roc_auc_score(y_test, y_prob)\n",
    "# but for binary classification\n",
    "# output from model.predict_proba() is a matrix with 2 columns, one for each class\n",
    "# to calculate roc, we need to provide the probability of the positive class:\n",
    "#     roc_auc_score(y_test, y_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# note: for feature_importance_permutation\n",
    "# metric MUST be \"r2\" for regression \n",
    "#        or \"accuracy\" for classification \n",
    "#        or a custom function with signature func(y_true, y_pred)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def ovr_roc_auc(y_true, y_pred):\n",
    "    if twoclass:\n",
    "        return roc_auc_score(y_true, y_pred[:,1], \n",
    "                             average='macro')\n",
    "    else:\n",
    "        return roc_auc_score(y_true, y_pred,\n",
    "                             multi_class='ovr',\n",
    "                             average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import feature_importance_permutation\n",
    "clf = models[0][1]\n",
    "clf.fit(X_train.values, ynum)\n",
    "\n",
    "imp_vals, _ = feature_importance_permutation(\n",
    "    predict_method=clf.predict_proba,      ## see note\n",
    "    X=X_train.values,                      ## cannot use dataframe\n",
    "    y=ynum,                                ## numeric labels\n",
    "    metric=ovr_roc_auc,                    ## custom scorer\n",
    "    num_rounds=1, seed=1)\n",
    "\n",
    "#print(imp_vals)\n",
    "#zz = sorted(zip(imp_vals,cols),reverse=True)\n",
    "#list(zz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "midf = pd.DataFrame({'Name': X_train.columns, 'Score': imp_vals})\n",
    "#midf.head()\n",
    "\n",
    "nf = 8     # number of features\n",
    "# extract the top nf\n",
    "mihi = midf.sort_values('Score', ascending=False).head(nf)\n",
    "#mihi\n",
    "\n",
    "nf = 8     # number of features\n",
    "# extract the low nf\n",
    "milo = midf.sort_values('Score', ascending=True).head(nf)\n",
    "#milo\n",
    "\n",
    "# merge\n",
    "hilo = pd.concat([mihi, milo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hilo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick sns.barplot\n",
    "ptitle = models[0][0]\n",
    "ptitle += \": feature importance via permutation\"\n",
    "sns.barplot(x = \"Score\", y = \"Name\", data = hilo).set(title=ptitle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ALL feature importances\n",
    "ptitle = models[0][0]\n",
    "ptitle += \": feature importance via permutation\"\n",
    "indices = np.flip(np.argsort(imp_vals))\n",
    "plt.figure()\n",
    "plt.title(ptitle)\n",
    "plt.bar(range(X_train.shape[1]), imp_vals[indices], color=\"b\")\n",
    "plt.xticks(range(X_train.shape[1]), cols)\n",
    "plt.xlim([0, X_train.shape[1]])\n",
    "plt.yticks(np.arange(-0.02, 0.10, 0.02))\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**<br>SequentialFeatureSelector**\n",
    "<br>Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d. \n",
    "<br><br>\n",
    "The motivation behind feature selection algorithms is to automatically select a subset of features most relevant to the problem. The goal of feature selection is two-fold: We want to improve the computational efficiency and reduce the model's generalization error by removing irrelevant features or noise.\n",
    "<br><br>\n",
    "In a nutshell, SFAs remove or add one feature at a time based on the classifier performance until a feature subset of the desired size k is reached. The 'floating' algorithms have an additional exclusion or inclusion step to remove features once they were included (or excluded) so that a larger number of feature subset combinations can be sampled.\n",
    "<br><br>\n",
    "http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/ has tutorial videos, diagrams<br>\n",
    "http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/#sequentialfeatureselector documents the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For SFS we can group one-hot encoded features as a single feature\n",
    "# feature_groups : the features within a group are always selected together, never split\n",
    "# HOWEVER\n",
    "# the `feature_group` list must contain ALL features and\n",
    "# there should be no common feature betweeen any two distinct groups of features provided \n",
    "\n",
    "# from above:\n",
    "#      put the names into a python list - for pandas.get_dummies()\n",
    "#      categori = combined_df.select_dtypes(include=['object']).columns\n",
    "# ((there is probably a more elegant way to do this, but never mind ...))\n",
    "\n",
    "# create the groups of features\n",
    "g = []\n",
    "for c in categori.tolist():\n",
    "    n = []\n",
    "    r = X_train.columns.str.startswith(c)\n",
    "    for x in range(len(r)):\n",
    "        if r[x]:\n",
    "            n.append(x)\n",
    "    g.append(n)\n",
    "#print(g)\n",
    "\n",
    "# get the others and add each one as an individual list\n",
    "a = []\n",
    "w = X_train.columns.tolist()\n",
    "\n",
    "for x in range(len(w)):\n",
    "    a.append(x)\n",
    "\n",
    "r=a\n",
    "for x in range(len(g)):\n",
    "        q=list(set(r).difference(g[x]))\n",
    "        r=q\n",
    "\n",
    "for x in range(len(r)):\n",
    "    z=[]\n",
    "    z.append(x)\n",
    "    g.append(z)\n",
    "\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch - only for the demo (churn) notebook!!\n",
    "#     [Geography was not done automatically by pandas]\n",
    "g[0] = [6, 7, 8]\n",
    "g=g[:-3]\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "go_fwd = False\n",
    " \n",
    "sfs = SFS(models[0][1], \n",
    "          k_features='parsimonious', \n",
    "          forward=go_fwd, \n",
    "          floating=True,\n",
    "          cv=2,      # CVfolds\n",
    "          feature_groups=g,\n",
    "          scoring='roc_auc_ovr',\n",
    "          verbose=0, \n",
    "          n_jobs= -1)\n",
    "\n",
    "sfs = sfs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\n",
    "\n",
    "if go_fwd:\n",
    "    plt.title('Sequential [Forward] Selection (w. StdDev)')\n",
    "else:\n",
    "    plt.title('Sequential [Backward] Selection (w. StdDev)')\n",
    "    \n",
    "#plt.ylim([0.8, 1])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uncomment the line below to see a dataframe of all of the details\n",
    "#pd.DataFrame.from_dict(sfs.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframes with just those columns\n",
    "mXtrain = X_train.filter(sfs.k_feature_names_)\n",
    "mXtest = X_test.filter(sfs.k_feature_names_)\n",
    "\n",
    "mXtrain.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtrainOriginal = X_train\n",
    "XtestOriginal = X_test\n",
    "\n",
    "X_train = mXtrain\n",
    "X_test = mXtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the feature names\n",
    "cols = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    "**<br>Fit and Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
