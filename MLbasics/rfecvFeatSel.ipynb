{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **tomato juice dataset**\n",
    "<br>` 'quality' is the target feature for classification `\n",
    "<br>` the other features are chemical properties of our product `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_S34U5S-i69d"
   },
   "source": [
    "**Import the main libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "# supress all\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9IZetEZ8jQJm"
   },
   "source": [
    "**Import the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## file path: windows style\n",
    "df = pd.read_csv('..\\\\datasets\\\\tomatjus.csv')\n",
    "\n",
    "## file path: unix style\n",
    "#df = pd.read_csv('../datasets/tomatjus.csv')\n",
    "\n",
    "# shape method gives the dimensions of the dataset\n",
    "print('Dataset dimensions: {} rows, {} columns'.format(\n",
    "    df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**Data Preparation and EDA** (unique to this dataset)\n",
    "* _Check for missing values_\n",
    "* _Quick visual check of unique values_\n",
    "* _Split the classification feature out of the dataset_\n",
    "* _Check column names of categorical attributes ( for get_dummies() )_\n",
    "* _Check column names of numeric attributes ( for Scaling )_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnt=0\n",
    "print('Missing Values - ')\n",
    "for col in df.columns:\n",
    "    nnul = pd.notnull(df[col]) \n",
    "    if (len(nnul)!=len(df)):\n",
    "        cnt=cnt+1\n",
    "        print('\\t',col,':',(len(df)-len(nnul)),'null values')\n",
    "print('Total',cnt,'features with null values')\n",
    "\n",
    "# address missing values here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Quick visual check of unique values, deal with unique identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify columns with only one value \n",
    "# or with number of unique values == number of rows\n",
    "n_eq_one = []\n",
    "n_eq_all = []\n",
    "\n",
    "print('Unique value count (',df.shape[0],'Rows in the dataset )')\n",
    "for col in df.columns:\n",
    "    lc = len(df[col].unique())\n",
    "    print(col, ' ::> ', lc)\n",
    "    if lc == 1:\n",
    "        n_eq_one.append(df[col].name)\n",
    "    if lc == df.shape[0]:\n",
    "        n_eq_all.append(df[col].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop columns with only one value\n",
    "if len(n_eq_one) > 0:\n",
    "    print('Dropping single-valued features')\n",
    "    print(n_eq_one)\n",
    "    data.drop(n_eq_one, axis=1, inplace=True)\n",
    "\n",
    "# Drop or bin columns with number of unique values == number of rows\n",
    "if len(n_eq_all) > 0:\n",
    "    print('Dropping unique identifiers')\n",
    "    print(n_eq_all)\n",
    "    data.drop(n_eq_all, axis=1, inplace=True)\n",
    "\n",
    "# continue with featue selection / feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Classification target feature**\n",
    "<br>\"the Right Answers\", or more formally \"the desired outcome\"\n",
    "<br>Must be in a separate dataset for classification ,,,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Make it a multi-class problem, using text labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  divide into classes by giving a range for quality\n",
    "##  Make it a multi-class problem: {3,4,5} {6} {7.8}\n",
    "bins = (2, 5, 6, 8)\n",
    "group_names = ['Average', 'Premium', 'Special']\n",
    "df['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the classification feature out of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature being predicted (\"the Right Answer\")\n",
    "labels_col = 'quality'\n",
    "y = df[labels_col]\n",
    "\n",
    "## Features used for prediction \n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy from slice\n",
    "# so we force it to create a new dataframe \n",
    "X = df.copy()\n",
    "X.drop(labels_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sorted list of unique labels to use later\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "targetlabels = unique_labels(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check column names of categorical attributes**\n",
    "<br>Features with text values (categorical attributes) need to be normalised\n",
    "<br>by changing them to numeric types that the algorithms find easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categori = X.select_dtypes(include=['object','category']).columns\n",
    "print(categori.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check column names of numeric attributes**\n",
    "<br>Features with numeric values need to be normalised\n",
    "<br>by changing them to small numbers in a specific range (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeri = X.select_dtypes(include=['float64','int64']).columns\n",
    "print(numeri.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proper place to do scaling comes later in the pipeline ,,, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "***\n",
    "**<br>Create Test // Train Datasets**\n",
    "> Split X and y datasets into Train and Test subsets,<br>keeping relative proportions of each class (stratify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=50, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<br>Target Label Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape method gives the dimensions of the dataset\n",
    "print('X_train: {} rows, {} columns'.format(X_train.shape[0], X_train.shape[1]))\n",
    "print('X_test:  {} rows, {} columns'.format(X_test.shape[0], X_test.shape[1]))\n",
    "print()\n",
    "print('y_train: {} rows, 1 column'.format(y_train.shape[0]))\n",
    "print('y_test:  {} rows, 1 column'.format(y_test.shape[0]))\n",
    "print()\n",
    "\n",
    "## Here's a nice report:  \n",
    "# 1. series to dataframe conversion\n",
    "my_train = pd.DataFrame(y_train)\n",
    "my_test = pd.DataFrame(y_test)\n",
    "# 2. dataframe copy with [[ -- ]]\n",
    "av_train = my_train[[labels_col]].apply(lambda x: x.value_counts())\n",
    "av_test = my_test[[labels_col]].apply(lambda x: x.value_counts())\n",
    "# 3. add a new column\n",
    "av_train['pct_train'] = round((100 * av_train / av_train.sum()),2)\n",
    "av_test['pct_test'] = round((100 * av_test / av_test.sum()),2)\n",
    "# 4. combine the dataframes\n",
    "av_tt = pd.concat([av_train,av_test], axis=1) \n",
    "# 5. print the report\n",
    "print('Frequency and Distribution of labels')\n",
    "print(av_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Next are standard steps for all datasets: _scaling, classifiers, results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**Scaling** comes _after_ test // train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the Numeric columns \n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# StandardScaler range: -1 to 1, MinMaxScaler range: zero to 1\n",
    "# ColumnTransformer returns a numpy.ndarray so we lose the feature names;\n",
    "# we process one column at a time to preserve the dataframe\n",
    "\n",
    "# sklearn docs say \n",
    "#   \"Don't cheat - fit only on training data, then transform both\"\n",
    "#   fit() expects 2D array: reshape(-1, 1) for single col or (1, -1) single row\n",
    "\n",
    "for i in numeri:\n",
    "    arr = np.array(X_train[i])\n",
    "    scale = MinMaxScaler().fit(arr.reshape(-1, 1))\n",
    "    X_train[i] = scale.transform(arr.reshape(len(arr),1))\n",
    "\n",
    "    arr = np.array(X_test[i])\n",
    "    X_test[i] = scale.transform(arr.reshape(len(arr),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Classifier Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature engineering process involves selecting the minimum \n",
    "   required features to produce a valid model because the more \n",
    "   features a model contains, the more complex it is (and the \n",
    "   more sparse the data), therefore the more sensitive the model \n",
    "   is to errors due to variance. \n",
    "   \n",
    "A common approach to eliminating features is to find their relative \n",
    "   importance, then eliminate weak features or combinations of \n",
    "   features and re-evalute to see if the model fares better during \n",
    "   cross-validation\n",
    "   \n",
    "In scikit-learn, tree models and ensembles of trees provide a \n",
    "   `feature_importances_` attribute when fitted, and linear models provide a `coef_` attribute when fitted.\n",
    "<br><br>\n",
    "`feature_importances_`\n",
    "> `sklearn.ensemble.AdaBoostClassifier()`\n",
    "  `sklearn.ensemble.ExtraTreesClassifier()`\n",
    "  `sklearn.ensemble.GradientBoostingClassifier()`\n",
    "  `sklearn.ensemble.RandomForestClassifier()`\n",
    "  `sklearn.tree.DecisionTreeClassifier()`\n",
    "  `sklearn.tree.ExtraTreeClassifier()`\n",
    "\n",
    "`coef_`\n",
    "> `sklearn.linear_model.LogisticRegression()`\n",
    "  `sklearn.linear_model.RidgeClassifier()`\n",
    "  `sklearn.linear_model.SGDClassifier()`\n",
    "  `sklearn.discriminant_analysis.LinearDiscriminantAnalysis()`\n",
    "  `sklearn.svm.LinearSVC()`\n",
    "   \n",
    "NOTE: `feature_importances_` and `coef_` can be misleading for \n",
    "      high cardinality features (many unique values). \n",
    "      The `permutation_importance` function works with any classifier, \n",
    "      and is an alternative in these cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare list\n",
    "models = []\n",
    "\n",
    "## feature_importances_\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#models.append(('DecisionTree', DecisionTreeClassifier()))\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "models.append(('RandomForest', RandomForestClassifier()))\n",
    "#from sklearn.ensemble import AdaBoostClassifier\n",
    "#models.append(('AdaBoostClassifier', AdaBoostClassifier()))\n",
    "#from sklearn.ensemble import GradientBoostingClassifier\n",
    "#models.append(('GradientBoostingClassifier', GradientBoostingClassifier()))\n",
    "\n",
    "## coef_\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#models.append(('LogisticRegression', LogisticRegression()))\n",
    "#from sklearn.linear_model import SGDClassifier\n",
    "#models.append(('StochasticGradientDescent', SGDClassifier()))\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "models.append(('LinearDiscriminantAnalysis', LinearDiscriminantAnalysis()))\n",
    "#from sklearn.svm import SVC\n",
    "#models.append(('SupportVectorClassifier', SVC()))\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg",
    "tags": []
   },
   "source": [
    " ***\n",
    " **_These examples only work with one classifier_** for example\n",
    ">models[0][1]  <br>models[1][1]  <br>models[2][1]\n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Feature Importance Visualisation**\n",
    "<br>Generally these viualisers incorporate `classifier.fit()`\n",
    "or want it immediately before,\n",
    "<br>so prior predictions are not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list of the feature names\n",
    "cols = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Yellowbrick visualizer uses `feature_importances_` or `coef_`  to rank and plot importance according to the explained variance each feature contributes to the model. \n",
    "* Using `feature_importances_` it is usual to plot relative importance, \n",
    "   where the most important feature is 100%, \n",
    "   and the rest are relative percent of the most important feature.\n",
    "* Using `coef_` it is better to set `relative=False` \n",
    "   to draw the true magnitude of the coefficient (which may be negative). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import feature_importances\n",
    "\n",
    "clf = models[0][1]\n",
    "# Use the quick method and immediately show the figure\n",
    "viz = feature_importances(clf, X_train, y_train, labels=cols, relative=False)\n",
    "# default: relative=True\n",
    "viz = feature_importances(clf, X_train, y_train, labels=cols, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>Stacked Feature Importances**\n",
    "<br>This one only works with classifiers that return `coef_`\n",
    "<br>`feature_importances_` is always an array of shape `n_features`\n",
    "<br><br>\n",
    "`coef_` is the same for binary classification, but in the multiclass \n",
    "   case it is an array of shape `n_classes, n_features` so the \n",
    "   relative importance of the feature to the prediction of the \n",
    "   probability of a specific class can be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "clf = models[1][1]\n",
    "\n",
    "viz = FeatureImportances(clf, stack=True, labels=cols, relative=False)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.show()\n",
    "\n",
    "viz = FeatureImportances(clf, stack=True, labels=cols, topn=5)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cpzuyj7gxwCg"
   },
   "source": [
    "**<br>RFE with CV**\n",
    "<br>ALWAYS EXPECT CROSS VALIDATION TO TAKE SOME TIME\n",
    "<br>Recursive Feature Elimination (RFE) is a feature selection method \n",
    "   that fits a model and removes the weakest feature (or features) \n",
    "   until the specified number of features is reached. Features are ranked by the modelâ€™s `feature_importances_` or `coef_`\n",
    "   attribute, and by recursively eliminating a small number of \n",
    "   features per loop, RFE attempts to eliminate dependencies and \n",
    "   collinearity that may exist in the model. \n",
    "   \n",
    "   To find the optimal number of features, the RFECV visualizer uses cross-validation to score different feature subsets and select the best scoring collection of features, and plots the number of features in the model along with \n",
    "their cross-validated test score and variability (shows some trash in spite of filterwarnings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import rfecv\n",
    "\n",
    "clf = models[0][1]\n",
    "crosval = StratifiedKFold(shuffle=True, random_state = 11)\n",
    "\n",
    "# Create the validation curve visualizer\n",
    "viz = rfecv(clf, X_train, y_train, \n",
    "            cv=crosval, scoring='f1_weighted', n_jobs= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know which features are being kept we can use the support_ attribute.\n",
    "viz.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the ranking from best (1) to worst, check the ranking_ attribute.\n",
    "viz.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe column index of selected features \n",
    "colndx = viz.get_support(indices=True)\n",
    "colndx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use a dataframe for display\n",
    "features_kept = pd.DataFrame({'columns': X_train.columns,\n",
    "                             'Kept': viz.support_})\n",
    "features_kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe using only the selected features\n",
    "# X_RFE = X_train.iloc[:, viz.support_]\n",
    "X_RFE = X_train.iloc[colndx]\n",
    "X_RFE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
