{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Normalising KDD and NSL datasets**\n",
    "<br>` python  3.7.13    scikit-learn  1.0.2 `\n",
    "<br>`numpy   1.19.5          pandas  1.3.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  pseudo-menu: uncomment *one* dataset  ###\n",
    "#dataset = 'KDD99 full train and test sets'\n",
    "#dataset = 'KDD99 10% train subset and full test set'\n",
    "#dataset = 'NSL_KDD+ full train and test sets'\n",
    "dataset = 'NSL_x21 full train set and difficult test subset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python does not have a 'switch // case' construct, must use nested 'if'\n",
    "# python slice    s = str[ start_pos : start_pos + substring_LENGTH]\n",
    "import os\n",
    "\n",
    "try:\n",
    "    dataset\n",
    "except NameError:\n",
    "    print(\"Using default datasets\")\n",
    "    dataset = 'KDD99 10% train subset and full test set'\n",
    "\n",
    "#dataset_root = '../datasets/original'\n",
    "dataset_root = os.path.join('..', 'datasets')\n",
    "dataset_root = os.path.join(dataset_root, 'original')\n",
    "\n",
    "if dataset[:3] == 'KDD':  \n",
    "    test_file = os.path.join(dataset_root, 'corrected')\n",
    "    if dataset[6:9] == '10%':\n",
    "        train_file = os.path.join(dataset_root, 'kddcup.data_10_percent_corrected')\n",
    "    else:\n",
    "        train_file = os.path.join(dataset_root, 'kddcup.data.corrected')\n",
    "else:  \n",
    "    train_file = os.path.join(dataset_root, 'NSL_KDDtrain.txt')\n",
    "    if dataset[4:7] == 'KDD':\n",
    "        test_file = os.path.join(dataset_root, 'NSL_KDDtest.txt')\n",
    "    else:\n",
    "        test_file = os.path.join(dataset_root, 'NSL_KDDtest-no21.txt')\n",
    "\n",
    "names_file = os.path.join(dataset_root, 'kddcup.names')\n",
    "ataks_file = os.path.join(dataset_root, 'training_attack_types')\n",
    "\n",
    "print('Train dataset: ' + train_file)\n",
    "print('Test dataset: ' + test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDD dataset features: http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\n",
    "# Target for classification is 'label', NSL_KDD adds 'difficulty'\n",
    "\n",
    "# initialise test and train set\n",
    "# first, read in the column names\n",
    "import csv\n",
    "with open(names_file) as f:\n",
    "    reader = csv.reader(f,delimiter = ':')\n",
    "    class_label = next(reader,None)\n",
    "    headerRow = [column[0] for column in reader]\n",
    "\n",
    "headerRow = headerRow + ['label']\n",
    "if dataset[:3] == 'NSL':\n",
    "    headerRow = headerRow + ['difficulty']\n",
    "\n",
    "# next, read in the data files and add the feature names for the columns\n",
    "train_df = pandas.read_csv(train_file, header=None)\n",
    "train_df.columns = headerRow\n",
    "test_df = pandas.read_csv(test_file, header=None)\n",
    "test_df.columns = headerRow    \n",
    "\n",
    "print('Train Dataset: {} rows, {} columns'.format(train_df.shape[0], train_df.shape[1]))\n",
    "print('Test Dataset: {} rows, {} columns'.format(test_df.shape[0], test_df.shape[1]))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the train and test sets are predfined, \n",
    "# we combine them to drop rows and convert text fields, then split them up again\n",
    "combined_df_raw = pandas.concat([train_df, test_df])\n",
    "\n",
    "# NSL-KDD has an extra field \n",
    "if dataset[:3] == 'NSL':\n",
    "    combined_df_raw.drop(['difficulty'], axis=1, inplace=True)\n",
    "\n",
    "combined_df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-class: Reduce the detailed attack labels to 'normal' or 'attack\n",
    "# Multiclass: Map each of the different attacks into 1 of 4 categories\n",
    "# http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types\n",
    "# has the 22 types in the train_set, there are 17 more types in the test_set\n",
    "\n",
    "from collections import defaultdict\n",
    "category = defaultdict(list)\n",
    "category['benign'].append('normal')   # better for alphabetical order\n",
    "\n",
    "# ataks_file = os.path.join(dataset_root, 'training_attack_types')\n",
    "with open(ataks_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "# skip blank lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        else:\n",
    "            attack, cat = line.strip().split(' ')\n",
    "            category[cat].append(attack)\n",
    "\n",
    "print('Categories with their attacks')\n",
    "print(category)\n",
    "# transform into a standard python dict\n",
    "atakmap = dict((v,k) for k in category for v in category[k])\n",
    "print('\\n' + 'Attacks with their category')\n",
    "print(atakmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add categories for the 17 attack types only in the test set\n",
    "# Thanks to \n",
    "# https://github.com/dimtics/Network-Intrusion-Detection-Using-Machine-Learning-Techniques\n",
    "\n",
    "testdf_only = {\n",
    "                'saint': 'probe',\n",
    "                'mscan': 'probe',\n",
    "                'mailbomb': 'dos',\n",
    "                'udpstorm': 'dos',\n",
    "                'apache2': 'dos',\n",
    "                'processtable': 'dos',\n",
    "                'xterm': 'u2r',\n",
    "                'ps': 'u2r',\n",
    "                'sqlattack': 'u2r',\n",
    "                'httptunnel': 'u2r',\n",
    "                'named': 'r2l',\n",
    "                'snmpguess': 'r2l',\n",
    "                'worm': 'r2l',\n",
    "                'snmpgetattack': 'r2l',\n",
    "                'xsnoop': 'r2l',\n",
    "                'xlock': 'r2l',\n",
    "                'sendmail': 'r2l'\n",
    "            }\n",
    "atakmap.update(testdf_only)\n",
    "print(atakmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: KDD99 labels end with a dot, NSL_KDD do not\n",
    "if dataset[:3] == 'KDD':\n",
    "    combined_df_raw['label'] = combined_df_raw['label'].str.strip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add the field \n",
    "combined_df_raw['atakcat'] = combined_df_raw['label'].map(atakmap)\n",
    "combined_df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_gQ0bLF-PHe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df_raw['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_gQ0bLF-PHe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined_df_raw['atakcat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the train // test split: slice 1 Dataframe into 2 \n",
    "# pandas has a lot of rules about returning a 'view' vs. a copy from slice\n",
    "# so we force it to create a new dataframe [avoiding SettingWithCopy Warning]\n",
    "pp_train = combined_df_raw.iloc[:len(train_df),:].copy()\n",
    "pp_test = combined_df_raw.iloc[len(train_df):,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_gQ0bLF-PHe"
   },
   "outputs": [],
   "source": [
    "pp_train['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_gQ0bLF-PHe"
   },
   "outputs": [],
   "source": [
    "pp_test['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_gQ0bLF-PHe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp_train['atakcat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_gQ0bLF-PHe"
   },
   "outputs": [],
   "source": [
    "pp_test['atakcat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python does not have a 'switch // case' construct, must use nested 'if'\n",
    "# python slice    s = str[ start_pos : start_pos + substring_LENGTH]\n",
    "\n",
    "#save as csv\n",
    "#df.to_csv(r'Path where you want to store the exported CSV file\\File Name.csv')\n",
    "\n",
    "#ppcsv_root = '../datasets/NSL_KDD'\n",
    "ppcsv_root = os.path.join('..', 'datasets')\n",
    "ppcsv_root = os.path.join(ppcsv_root, 'NSL_KDD')\n",
    "\n",
    "if dataset[:3] == 'KDD':  \n",
    "    test_csv = os.path.join(ppcsv_root, 'KDD_ppTest.csv')\n",
    "    if dataset[6:9] == '10%':\n",
    "        train_csv = os.path.join(ppcsv_root, 'KDD_ppTrain_10pct.csv')\n",
    "    else:\n",
    "        train_csv = os.path.join(ppcsv_root, 'KDD_ppTrain_full.csv')\n",
    "else:  \n",
    "    train_csv = os.path.join(ppcsv_root, 'NSL_ppTrain.csv')\n",
    "    if dataset[4:7] == 'KDD':\n",
    "        test_csv = os.path.join(ppcsv_root, 'NSL_ppTest.csv')\n",
    "    else:\n",
    "        test_csv = os.path.join(ppcsv_root, 'NSL_ppTest-no21.csv')\n",
    "\n",
    "print('Saving')\n",
    "print('Train dataset: ' + train_csv)\n",
    "print('Test dataset: ' + test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_train.to_csv(train_csv, index = False)\n",
    "pp_test.to_csv(test_csv, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
